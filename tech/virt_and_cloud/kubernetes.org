#+TAGS: devops virtualization orchestration kubernetes


* Kubernetes
HomePage: [[https://kubernetes.io/][kubernetes.io]]
Documentation: [[https://kubernetes.io/docs/home/?path=users&persona=app-developer&level=foundational][kubernetes.io/docs]]
List of Materials: [[https://github.com/ramitsurana/awesome-kubernetes][github.com/awesome-kubernetes]]

* Files
/var/log/containers - this holds all the logs for each pod, and are ascii files that can be accessed with standard tools (cat, grep etc)
* Cmds
- kubectl
- kubecfg
- minikube - tool for creating a single node kubernetes deployment on local workstation
- Flannel - https://coreos.com/flannel/docs/latest/kubernetes.html

* Description
** Components
- Master - the managing machine, which oversees one or more minions.
- Minion - a slave that runs tasks as delegated by the user and Kubernetes master.
- Pod - an application (or part of an application) that runs on a minion. This is the basic unit of manipulation in Kubernetes.
- Replication Controller - ensures that the requested number of pods are running on minions at all times.
- Label - an arbitrary key/value pair that the Replication Controller uses for service discovery
- Kubecfg - the command line config tool
- Service - an endpoint that provides load balancing across a replicated group of pods

* Usage
- Overview of the cluster
#+BEGIN_SRC sh
kubectl cluster-info
#+END_SRC

- Pods overview
#+BEGIN_SRC sh
kubectl get pods -o wide
#+END_SRC

- starting a pod
#+BEGIN_SRC sh
kubectl run name-of-deployment --image=nginx:latest --port=8080
#+END_SRC

- accessing a container inside a pod with other containers
#+BEGIN_SRC sh
kubectl exec -it star-aaaaaaaaaa-bbbbb --container sidecar1 -- /bin/bash
#+END_SRC

** Kubectl
*** get - list resources
- list all nodes
#+BEGIN_SRC sh
kubectl get nodes
#+END_SRC

- check for pods on a node
#+BEGIN_SRC sh
kubectl get pods --all-namespaces -o wide
#+END_SRC

- what pods are running in the namespace
#+BEGIN_SRC sh
kubectl get pods -n kube-system
#+END_SRC

- list the current running deployments
#+BEGIN_SRC sh
kubectl get deployments
#+END_SRC

*** describe - show detailed information about a resource
- check for pods on a node
#+BEGIN_SRC sh
kubectl describe node node-name
#+END_SRC

- get a detailed overview of a node
#+BEGIN_SRC sh
kubectl describe node fredflinstone1.mylabserver.com | less
#+END_SRC
this provides lots of information, such as OS information, disk usage, memory usage etc

- get detailed information on a service
#+BEGIN_SRC sh
kubectl describe services/kubernetes-bootcamp
#+END_SRC

- get detailed detailed information on a deployment
#+BEGIN_SRC sh
kubectl describe deployment/kubernetes-bootcamp
#+END_SRC

*** logs - print the logs from a container in a pod
- view the log
#+BEGIN_SRC sh
kubectl logs config-test-pod
#+END_SRC

*** exec - execute a cmd on a container in a pod
- list the environment variables on a container
#+BEGIN_SRC sh
kubectl exec nginx-deployment-75675f5897-8ztt9 env
#+END_SRC

- start a bash shell in the container
#+BEGIN_SRC sh
kubectl exec nginx-deployment-75675f5897-8ztt9 bash
#+END_SRC




*** delete
- delete a pod
#+BEGIN_SRC sh
kubectl delete pod nginx-deployment-75675f5897-8ztt9
#+END_SRC
this will cause kubenetes to create a new pod, to change the pod number use "scale"

- delete a deployment
#+BEGIN_SRC sh
kubectl delete deployment kubernetes-bootcamp
#+END_SRC

- delete a service
#+BEGIN_SRC sh
kubectl delete service -l run=kubernetes-bootcamp
#+END_SRC
this will only remove the service, the deployment will still be available

*** scale
- change the current number of replicas to 4
#+BEGIN_SRC sh
kubectl scale deployments/kubernetes-bootcamp --replicas=4
#+END_SRC

** Install Kubernetes on Ubuntu 16.04
*** Master Node
- first update the system
#+BEGIN_SRC sh
apt-get update && apt-get upgrade
#+END_SRC

- install docker 
#+BEGIN_SRC sh
apt-get install -y docker.io
#+END_SRC

- set docker to use systemd cgroupdriver
#+BEGIN_SRC sh
cat << EOF > /etc/docker/daemon.json
{
  "exec-opts: ["native.cgroupdriver=systemd"]
}
EOF
#+END_SRC

- install the gpg key for the repo
#+BEGIN_SRC sh
curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -
#+END_SRC

- add the kubernetes sources
#+BEGIN_SRC sh
cat << EOF > /etc/apt/sources.list.d/kubernetes.list
deb http://apt.kubernetes.io/ kubernetes-xenial main
EOF
#+END_SRC

- update to grab new sources
#+BEGIN_SRC sh
apt-get update
#+END_SRC

- install the 3 main pieces of kubernetes
#+BEGIN_SRC sh
apt-get install -y kubelet kubeadm kubectl
#+END_SRC

- initialize kubernetes
#+BEGIN_SRC sh
kubeadm init --pod-network-cidr=10.244.0.0/16
#+END_SRC
--pod-network-cidr option is set as we will be using flannel as our CNI(Container Network Interface)
- this will provide us with the cmd that is need to allow other nodes to join the cluster, save this in a file
  
- setting up kubernetes for the user
#+BEGIN_SRC sh
mkdir -p $HOME/.kube
cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
chown $(id -u):$(id -g) /home/user/.kube/config
#+END_SRC

- install the CNI flannel
#+BEGIN_SRC sh
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/v0.9.1/Documentation/kube-flannel.yml
#+END_SRC

- show all currently installed services on kubernetes
#+BEGIN_SRC sh
kubectl get pods --all-namespaces
#+END_SRC

*** Worker Node
- install docker 
#+BEGIN_SRC sh
apt-get install docker.io
#+END_SRC

- set docker to use systemd cgroupdriver
#+BEGIN_SRC sh
cat << EOF > /etc/docker/daemon.json
{
  "exec-opts: ["native.cgroupdriver=systemd"]
}
EOF
#+END_SRC

- install the gpg key for the repo
#+BEGIN_SRC sh
curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -
#+END_SRC

- add the kubernetes sources
#+BEGIN_SRC sh
cat << EOF > /etc/apt/sources.list.d/kubernetes.list
deb http://apt.kubernetes.io/ kubernetes-xenial main
EOF
#+END_SRC

- update to grab new sources
#+BEGIN_SRC sh
apt-get update
#+END_SRC

- install the 3 main pieces of kubernetes
#+BEGIN_SRC sh
apt-get install -y kubelet kubeadm kubectl
#+END_SRC
kubectl can be installed on the worker node but isn't required, but will allow cmds to be issued from this node
 
- join the worker to the master
#+BEGIN_SRC sh
kubeadm join 172.31.19.206:6443 --token v89vru.yypa0p30j8j2bgqx --discovery-token-ca-cert-hash sha256:d99c4ca5c79c14c4505f9791eb3833e25e291ff91ff82bc1102790980468fa5a
#+END_SRC

- confirm on the master node that the worker has joined
#+BEGIN_SRC sh
kubectl get nodes
#+END_SRC

** Install Kubernetes on Centos7
*** Master Node
- disable the swap partition
#+BEGIN_SRC sh
swapoff -a
#+END_SRC
remove or comment out any swap entries in /etc/fstab

- update the system
#+BEGIN_SRC sh
yum update -y
#+END_SRC

- install required packages for docker
#+BEGIN_SRC sh
yum install -y device-mapper-persistent-data lvm2 yum-utils
#+END_SRC
yum-utils is for yum-config-manager

- add the stable repo
#+BEGIN_SRC sh
yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo
yum update
#+END_SRC

- for bledding edge
#+BEGIN_SRC sh
yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo --enable docker-ce-edge.repo
yum update
#+END_SRC

- install docker
#+BEGIN_SRC sh
yum install -y docker-ce
#+END_SRC

- add users that require to access docker to the docker group 
#+BEGIN_SRC sh
usermod -aG docker user1
usermod -aG docker user2
usermod -aG docker user3
#+END_SRC
access is required to the /var/run/docker.sock file that has perms root:docker

- enable docker service
#+BEGIN_SRC sh
systemctl enable docker.service
systemctl start docker.service
systemctl status docker.service
#+END_SRC

- add the kubernetes repo
#+BEGIN_SRC sh
cat <<EOf > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
EOF
#+END_SRC

- disable selinux
#+BEGIN_SRC sh
setenforce 0
#+END_SRC

- set selinux to permissive
/etc/selinux/config
#+BEGIN_EXAMPLE
SELINUX=premissive
#+END_EXAMPLE

- install the parts of kubernetes
#+BEGIN_SRC sh
yum install -y kubelet kubeadm kubectl
#+END_SRC

- start and enable kubelet
#+BEGIN_SRC sh
systemctl enable kubelet
systemctl start kubelet
systemctl status kubelet
#+END_SRC

- edit /etc/sysctl.d/k8s.conf
#+BEGIN_SRC sh
cat << EOF > /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF
#+END_SRC

- enable the k8s.conf
#+BEGIN_SRC sh
sysctl --system
#+END_SRC

- initialize the kubernetes cluster
#+BEGIN_SRC sh
kubeadm init --pod-network-cidr=10.244.0.0/16
#+END_SRC
save the token provided at the end of the setup

- configure kubernetes for the user
#+BEGIN_SRC sh
mkdir -p $HOME/.kube
cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
choen $(id -u):$(id -g) $HOME/.kube/config
#+END_SRC

- install flannel
#+BEGIN_SRC sh
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/v0.9.1/Documentation/kube-flannel.yml
#+END_SRC

- confirm that the master is ready for workers to join
#+BEGIN_SRC sh
kubectl get nodes
#+END_SRC
STATUS should state "Ready"

*** Worker Node 
- disable the swap partition
#+BEGIN_SRC sh
swapoff -a
#+END_SRC
remove or comment out any swap entries in /etc/fstab

- update the system
#+BEGIN_SRC sh
yum update -y
#+END_SRC

- install required packages for docker
#+BEGIN_SRC sh
yum install -y device-mapper-persistent-data lvm2 yum-utils
#+END_SRC
yum-utils is for yum-config-manager

- add the stable repo
#+BEGIN_SRC sh
yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo
yum update
#+END_SRC

- for bledding edge
#+BEGIN_SRC sh
yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo --enable docker-ce-edge.repo
yum update
#+END_SRC

- install docker
#+BEGIN_SRC sh
yum install -y docker-ce
#+END_SRC

- add users that require to access docker to the docker group 
#+BEGIN_SRC sh
usermod -aG docker user1
usermod -aG docker user2
usermod -aG docker user3
#+END_SRC
access is required to the /var/run/docker.sock file that has perms root:docker

- enable docker service
#+BEGIN_SRC sh
systemctl enable docker.service
systemctl start docker.service
systemctl status docker.service
#+END_SRC

- add the kubernetes repo
#+BEGIN_SRC sh
cat <<EOf > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
EOF
#+END_SRC

- disable selinux
#+BEGIN_SRC sh
setenforce 0
#+END_SRC

- set selinux to permissive
/etc/selinux/config
#+BEGIN_EXAMPLE
SELINUX=premissive
#+END_EXAMPLE

- install the parts of kubernetes
#+BEGIN_SRC sh
yum install -y kubelet kubeadm kubectl
#+END_SRC

- start and enable kubelet
#+BEGIN_SRC sh
systemctl enable kubelet
systemctl start kubelet
systemctl status kubelet
#+END_SRC

- edit /etc/sysctl.d/k8s.conf
#+BEGIN_SRC sh
cat << EOF > /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF
#+END_SRC

- enable the k8s.conf
#+BEGIN_SRC sh
sysctl --system
#+END_SRC

- connect other nodes to the cluster, this is done on the other node
#+BEGIN_SRC sh
kubeadm join 172.31.19.206:6443 --token v89vru.yypa0p30j8j2bgqx --discovery-token-ca-cert-hash sha256:d99c4ca5c79c14c4505f9791eb3833e25e291ff91ff82bc1102790980468fa5a
#+END_SRC

- confirm on the master that the node has joined
#+BEGIN_SRC sh
kubectl get nodes
#+END_SRC
this will print all nodes that are currently in the cluster

** Setting up a cluster
- initialise the master
#+BEGIN_SRC sh
kubeadm init -- pod-network-cidr=10.244.0.0/16
#+END_SRC
this will create the token needed to allow nodes to join the cluster

- configure the user
#+BEGIN_SRC sh
mkdir -p $HOME/.kube
cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
chown $(id -u):$(id -g) $HOME/.kube/config
#+END_SRC

- install the networking package flannel
#+BEGIN_SRC sh
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/v0.9.1/Documentation/kube-flannel.yml
#+END_SRC
this provides networking between the pods

- view everything that has been created in the cluster so far
#+BEGIN_SRC sh
kubectl get pods --all-namespaces
#+END_SRC

- connect other nodes to the cluster, this is done on the other node
#+BEGIN_SRC sh
kubeadm join 172.31.19.206:6443 --token v89vru.yypa0p30j8j2bgqx --discovery-token-ca-cert-hash sha256:d99c4ca5c79c14c4505f9791eb3833e25e291ff91ff82bc1102790980468fa5a
#+END_SRC

- confirm on the master that the node has joined
#+BEGIN_SRC sh
kubectl get nodes
#+END_SRC
this will print all nodes that are currently in the cluster

** Required Ports
Master node:
  - TCP 6443      Kubernetes API server
  - TCP 2379-2380 etcd server client API
  - TCP 10250     Kubelet API
  - TCP 10251     kube-scheduler
  - TCP 10252     kube-controller-manager
  - TCP 10255     Read-Only Kubelet API
    
Worker nodes:
  - TCP 10250    Kubelet API
  - TCP 10255    Read-Only Kubelet API
  - TCP 30000-32767 NodePort Services

** Deployments, Rolling Updates and Rollbacks
- nginx-deployment.yml
#+BEGIN_EXAMPLE
---
apiVersion: apps/v1beta2
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  selector:
    matchLabels:
      app: nginx
  replicas: 2
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.7.9
        ports:
        - containerPort: 80
#+END_EXAMPLE

- deploy the yaml file
#+BEGIN_SRC sh
kubectl create -f nginx-deployment.yml
#+END_SRC

- confirm deployment
#+BEGIN_SRC sh
kubectl get deployments
#+END_SRC

- view more information on the deployment
#+BEGIN_SRC sh
kubectl describe deployment nginx-deployment
#+END_SRC

- have kubectl output a deployment in yaml format
#+BEGIN_SRC sh
kubectl get deployment nginx-deployment -o yaml 
#+END_SRC

- update an image in a deployment
#+BEGIN_SRC sh
kubectl set image deployment/nginx-deployment nginx=nginx:1.8
#+END_SRC
this will update nginx from 1.7.9 to 1.8

- view the rollout status
#+BEGIN_SRC sh
kubectl rollout status deployment/nginx-deployment
#+END_SRC

- update an image with a yml file
#+BEGIN_EXAMPLE
      containers:
      - name: nginx
        image: nginx:1.9.1
        ports:
        - containerPort: 80
#+END_EXAMPLE
just edit the image: tag to the image to be updated to

#+BEGIN_SRC sh
kubectl apply -f nginx-deployment.yml
#+END_SRC

- check the rollout history
#+BEGIN_SRC sh
kubectl rollout history deployment/nginx-deployment --revision=3
#+END_SRC

- rollback to a previous revision
#+BEGIN_SRC sh
kubectl rollout undo deployment/nginx-deployment --to-revision=2
#+END_SRC

** Configuration Maps
- create a configuration map
#+BEGIN_SRC sh
kubectl create configmap my-map --from-literal=school=LinuxAcademy
#+END_SRC

- confirm creation
#+BEGIN_SRC sh
kubectl get configmaps
#+END_SRC

- detailed view of configmap
#+BEGIN_SRC sh
kubectl describe configmaps my-map
#+END_SRC

- create a pod with configmap
pod-config.yml
#+BEGIN_EXAMPLE
---
apiVersion: v1
kind: Pod
metadata:
  name: config-test-pod
spec:
  containers:
    - name: test-container
      image: busybox
      command: [ "/bin/sh", "-c", "env" ]
      env:
        - name: WHAT_SCHOOL
          valueFrom:
            configMapKeyRef:
              name: my-map
              key: school
  restartPolicy: Never
#+END_EXAMPLE

- create the pod
#+BEGIN_SRC sh
kubectl create -f pod-config.yml
#+END_SRC

- view the output
#+BEGIN_SRC sh
kubectl get pods --show-all
#+END_SRC

- view the log
#+BEGIN_SRC sh
kubectl logs config-test-pod
#+END_SRC

** Scaling Applications
- adding replicas to deployment
#+BEGIN_SRC sh
kubectl scale deployment/nginx-deployment --replicas=3
#+END_SRC
this will change the current level of replicas to 3

- confirm the change 
#+BEGIN_SRC sh
kubectl get deployments
#+END_SRC

** Labels and Selecors
- setting a label to a pod
#+BEGIN_SRC sh
kubectl label pods mysql-544bbdcd6f-grtfk test=sure
#+END_SRC

- resetting a label to a pod
#+BEGIN_SRC sh
kubectl label pods mysql-544bbdcd6f-grtfk test=sure --overwrite
#+END_SRC
overwrite - this will reset a label

- label many pods with a label
#+BEGIN_SRC sh
kubectl label pods -l app=nginx tier=frontend
#+END_SRC

- delete pods using labels
#+BEGIN_SRC sh
kubectl delete pods -l test=sure
#+END_SRC

- adding a label to a node
#+BEGIN_SRC sh
kubectl label node fredflintstone1.mylabserver.com colour=red
#+END_SRC

- List all running pods in the default namespace that have the key/value pair database=mysql
#+BEGIN_SRC sh
kubectl get pods -l database=mysql -n default
#+END_SRC

** DaemonSets
Doc: [[https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/#updating-a-daemonset][kubernetes.io/daemonset]]
Special scheduling case, when a particular pods has to be run on every node (things like flannel, monitoring tools etc)

- to view them
#+BEGIN_SRC sh
kubectl get daemonsets -n kube-system
#+END_SRC
n - namespace

- view detailed information
#+BEGIN_SRC sh
kubectl describe daemonset kube-flannel-ds -n kube-system
#+END_SRC

- delete a daemonset
#+BEGIN_SRC sh
kubectl delete ds name_of_ds
kubectl delete daemonset name_of_ds
#+END_SRC

** Taints
- view any taints that are set on a node
#+BEGIN_SRC sh
kubectl describe node fredflinstone1.mylabserver.com | grep -i -B3 -A3 "taints"
#+END_SRC

- remove a taint
#+BEGIN_SRC sh
kubectl taint nodes fredflinstone1.mylabserver.com node-role.kubernetes.io/master-
#+END_SRC

- add a taint
#+BEGIN_SRC sh
kubectl taint nodes fredflinstone1.mylabserver.com node-role.kubernetes.io/master:NoSchedule
#+END_SRC

** Manually Scheduling a Pod
- label the node
#+BEGIN_SRC sh
kubectl label node fredflintstone3.mylabserver.com net=gigabit
#+END_SRC

- create the deployment that you are to schedule
#+BEGIN_EXAMPLE
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: webhead
spec:
  replicas: 1
  selector:
    matchLabels:
      run: webhead
  template:
    metadata:
      labels:
        run: webhead
    spec:
      containers:
      - image: nginx
        name: webhead
        port:
        - containerPort: 80
          portocol: TCP
      nodeSelector:
        net: gigabit
#+END_EXAMPLE

** Monitoring
Applications
  - heapset - cluster-wide aggregator of monitoring and event data
  - cAdvisor - is an open source contianer resource usage and performance analysis agent
  - Grafana with InfluxDB - these run in pods
  - Google Cloud Monitoring - hosted monitoring service
    
- accessing logs in pods
#+BEGIN_SRC sh
kubectl logs name_of_pod
#+END_SRC

- only view the last 15 lines of the log
#+BEGIN_SRC sh
kubectl logs name_of_pod --tail=15
#+END_SRC
tail - defaults to 10 with no number specified

** Upgrading Kubernetes without taking down the cluster
- upgrade kubeadm
#+BEGIN_SRC sh
sudo apt-get upgrade kubeadm
#+END_SRC
we can only update kubenetes to the version that kubeadm is at

- check the version of kubeadm
#+BEGIN_SRC sh
kubeadm version
#+END_SRC

- check what versions are currenlty available
#+BEGIN_SRC sh
kubeadm upgrade plan
#+END_SRC
this will provide a cmd that you can enter to update kubernetes

- run the upgrade
#+BEGIN_SRC sh
kubeadm upgrade apply v1.10.2
#+END_SRC

- this then needs to be done on all the other nodes

** Removing a node from the cluster
- drain the node of active pods
#+BEGIN_SRC sh
kubectl drain node_name --ignore-daemonsets
#+END_SRC

- delete the node from the cluster
#+BEGIN_SRC sh
kubectl delete node node_name
#+END_SRC

- view current token list
#+BEGIN_SRC sh
kubeadm token list
#+END_SRC

- generate a token for a new user
#+BEGIN_SRC sh
kubeadm token generate
kubeadm token create sty2qv.ovsxx34tfyxerv2n --ttl 3h --print-join-command
#+END_SRC
copy the cmd that is printed out

- on the new node that is to be added to the cluster run
#+BEGIN_SRC sh
kubeadm join 172.31.103.9:6443 --token sty2qv.ovsxx34tfyxerv2n --discovery-token-ca-cert-hash sha256:7c90998c210b785274dcdff611848094ecf59f8d1540cd8f50a7a0d0c454043f
#+END_SRC
 
** Networking
- binding deployment to host port
#+BEGIN_SRC sh
kubectl expose deployment webhead --type="NodePort" --port 80
#+END_SRC

- confirm what port the service bound to
#+BEGIN_SRC sh
kubectl get services
#+END_SRC

- confirm
#+BEGIN_SRC sh
w3m http://localhost:30589
#+END_SRC

*** Deploying a Load Balancer
- /service-lb.yml
#+BEGIN_EXAMPLE
---
apiVersion: v1
kind: Service
metadata:
  name: la-lb-service
spec:
  selector:
    app: la-lb
  ports:
  - protocol: TCP
    port: 80
    targetPort: 9376
  clusterIP: 10.0.171.223
  loadBalancerIP: 78.12.23.17
  type: LoadBalance
#+END_EXAMPLE

** Persistent Volumes
*** Example of awsEBS yaml
#+BEGIN_EXAMPLE
---
apiVersion: v1
kind: Pod
metadata:
  name: test-ebs
spec:
  containers:
  - images:
k8s.gcr.io/test-webserver
  name: test-container
  volumeMounts:
  - mountPath: /test-ebs
    name: test-volume
  volumes:
  - name: test-volume
    # This AWS EBS volume must
    # already exist.
    awsElasticBlockStore:
      volumeID: <volume-id>
      fsType: ext4
#+END_EXAMPLE
*** Example of emptyDir yaml
#+BEGIN_EXAMPLE
---
apiVersion: v1
kind: Pod
metadata:
  name: test-pd
spec:
  containers:
  - image: k8s.gcr.io/test-webserver
    name: test-container
    volumeMounts:
    - mountPath: /cache
      name: cache-volume
  volumes:
  - name: cache-volume
    emptyDir: {}
#+END_EXAMPLE
*** Example gcePersistenDisk yaml
#+BEGIN_EXAMPLE
apiVersion: v1
kind: Pod
metadata:
  name: test-pd
spec:
  containers:
  - image: k8s.gcr.io/test-webserver
    name: test-container
    volumeMounts:
    - mountPath: /test-pd
      name: test-volume
  volumes:
  - name: test-volume
    # This GCE PD must already exist
    gcePersistentDisk:
      pdName: my-data-disk
      fsType: ext4
#+END_EXAMPLE
*** Setting up an NFS share ubuntu 16
#+BEGIN_SRC sh
apt-get install nfs-kernel-server
#+END_SRC

- create the dir to share
#+BEGIN_SRC sh
mkdir /var/nfs/general
#+END_SRC

- set the permissions (this shouldn't be done on a production server)
#+BEGIN_SRC sh
chown nobody:nobody /var/nfs/general
#+END_SRC

- edit fstab
#+BEGIN_EXAMPLE
/var/nfs/general 172.31.103.9(rw,sync,no_subtree_check) 172.31.116.158(rw,sync,no_subtree_check) 172.31.16.102 (rw,sync,no_subtree_check) 172.31.113.129(rw,sync,no_subtree_check)
#+END_EXAMPLE

- install nfs-common on each node
#+BEGIN_SRC sh
apt-get install nfs-common
#+END_SRC

- provision our storage with a yml file
pv.yaml
#+BEGIN_EXAMPLE
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: lapv
spec:
  capacity:
    storage: 1Gi
  volumeMode: Filesystem
  accessModes:
    - ReadWriteMany
  persistentVolumeReclaimPolicy: Recycle
  nfs:
    path: /var/nfs/general
    server: 172.31.22.210
    readOnly: false
#+END_EXAMPLE
server - this will be the allocated private ip set for the LA VM

- have kubernetes create the persistent volume
#+BEGIN_SRC sh
kubectl create -f pv.yml
#+END_SRC

- confirm the pv creation
#+BEGIN_SRC sh
kubectl get pv
#+END_SRC

- claim the pv
pvc.yml
#+BEGIN_EXAMPLE
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: nfs-pvc
spec:
  accessModes:
  - ReadWriteMany
  resources:
    requests:
      storage: 1Gi
#+END_EXAMPLE

- create the claim
#+BEGIN_SRC sh
kubectl create -f pvc.yml
#+END_SRC

- confirm the claim
#+BEGIN_SRC sh
kubectl get pvc
#+END_SRC

- create a pod that will use the volume
nfs-pod.yml
#+BEGIN_EXAMPLE
---
apiVersion: v1
kind: Pod
metadata:
  name: nfs-pod
  labels:
    name: nfs-pod
spec:
  containers:
  - name: nfs-ctn
    image: busybox
    command:
      - sleep
      - "3600"
    volumeMounts:
    - name: nfsvol
      mountPath: /tmp
  restartPolicy: Always
  securityContext:
    fsGroup: 65534
    runAsUser: 65534
  volumes:
    - name: nfsvol
      persistentVolumeClaim:
        claimName: nfs-pvc
#+END_EXAMPLE

- create the pod
#+BEGIN_SRC sh
kubectl create -f nfs-pod.yml
#+END_SRC

- confirm the pod is running and the volume is attached
#+BEGIN_SRC sh
kubectl get pods -o wide
#+END_SRC

- connect the to the pod and create a test file in the mounted pvc
#+BEGIN_SRC sh
kubectl exec -it nfs-pod -- sh
cd /tmp
touch test-file.txt
#+END_SRC

- confirm on the NFS server that the file was created
#+BEGIN_SRC sh
cd /var/nfs/general
ls
#+END_SRC

** Security Contexts

** Setting up Horizontal Pod Autoscaling

- clone the metrics api for kubernetes
#+BEGIN_SRC sh
git clone https://github.com/kubernetes-incubator/metrics-server
#+END_SRC

- now build the deployment
#+BEGIN_SRC sh
cd metrics-server
kubectl create -f deploy/1.8+/
#+END_SRC

- confirm the build of the metrics api
#+BEGIN_EXAMPLE
cloud_user@ip-10-0-1-145:~/metrics-server$ kubectl get pods -n kube-system
NAME                                    READY     STATUS    RESTARTS   AGE
etcd-ip-10-0-1-145                      1/1       Running   0          30m
kube-apiserver-ip-10-0-1-145            1/1       Running   0          30m
kube-controller-manager-ip-10-0-1-145   1/1       Running   0          30m
kube-dns-86f4d74b45-b4fpp               3/3       Running   0          31m
kube-flannel-ds-gbgt7                   1/1       Running   1          31m
kube-flannel-ds-w4fv8                   1/1       Running   1          30m
kube-proxy-h6w78                        1/1       Running   0          30m
kube-proxy-j7xxl                        1/1       Running   0          31m
kube-scheduler-ip-10-0-1-145            1/1       Running   0          31m
#+END_EXAMPLE
we should see the "kube-apiserver-ip-10-0-1-145" 

- test that the api server can interact with k8s
#+BEGIN_EXAMPLE
cloud_user@ip-10-0-1-145:~/metrics-server$ kubectl get --raw /apis/metrics.k8s.io
{"kind":"APIGroup","apiVersion":"v1","name":"metrics.k8s.io","versions":[{"groupVersion":"metrics.k8s.io/v1beta1","version":"v1beta1"}],"preferredVersion":{"groupVersion":"metrics.k8s.io/v1beta1","version":"v1beta1"},"serverAddressByClientCIDRs":null}
#+END_EXAMPLE

- now edit the deployment file to include the autoscaling
#+BEGIN_EXAMPLE
kind: Service
apiVersion: v1
metadata:
  name: train-schedule-service
spec:
  type: NodePort
  selector:
    app: train-schedule
  ports:
  - protocol: TCP
    port: 8080
    nodePort: 8080

---

apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: train-schedule-deployment
  labels:
    app: train-schedule
spec:
  replicas: 2
  selector:
    matchLabels:
      app: train-schedule
  template:
    metadata:
      labels:
        app: train-schedule
    spec:
      containers:
      - name: train-schedule
        image: linuxacademycontent/train-schedule:autoscaling
        ports:
        - containerPort: 8080
        livenessProbe:
          httpGet:
            path: /
            port: 8080
          initialDelaySeconds: 15
          timeoutSeconds: 1
          periodSeconds: 10
        resources:
          requests:
            cpu: 200m

---

apiVersion: autoscaling/v2beta1
kind: HorizontalPodAutoscaler
metadata:
  name: train-schedule
  namespace: default
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: train-schedule-deployment
  minReplicas: 1
  maxReplicas: 4
  metrics:
  - type: Resource
    resource:
      name: cpu
      targetAverageUtilization: 50
#+END_EXAMPLE

- deploy the alerted file
#+BEGIN_SRC sh
kubectl apply -f train-schedule-kube.yml
#+END_SRC

- confirm that min and max have been set and that cpu usage is being registered
#+BEGIN_SRC sh
kubectl get hpa -w
#+END_SRC

Run this within another terminal
- test the autoscaling with busybox cluster hitting web address to simulate traffic
#+BEGIN_SRC sh
kubectl run -i --tty load-generator --image=busybox /bin/sh
#+END_SRC

- this will drop us into a shell in the busybox pod run a while loop to simulate traffic
#+BEGIN_SRC sh
while true; do wget -q -O- http://121.34.8.21:8080/generate-cpu-load; done
#+END_SRC
that will be the nodes ip address that contains the pods

- on the master you will notice the cpu load increase and autoscaling should start to provision more repicas in response
#+BEGIN_EXAMPLE
cloud_user@ip-10-0-1-145:~$ kubectl get hpa -w
NAME             REFERENCE                              TARGETS   MINPODS   MAXPODS   REPLICAS   AGE
train-schedule   Deployment/train-schedule-deployment   0%/50%    1         4         1          31m
train-schedule   Deployment/train-schedule-deployment   107%/50%   1         4         1         32m
train-schedule   Deployment/train-schedule-deployment   72%/50%    1         4         3         33m
#+END_EXAMPLE
notice the increase in replicas due to the increase in cpu load






* Lecture
* Tutorial
* Books
** [[file://home/crito/Documents/SysAdmin/Cloud/Getting_Started_with_Kubernetes.pdf][Getting Started with Kubernetes]]

* Links
https://www.techrepublic.com/article/how-to-quickly-install-kubernetes-on-ubuntu/
