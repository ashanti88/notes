#+TAGS: cloud aws vpc virtula_private_cloud iaas paas


* Amazon Web Services
HomePage: https://aws.amazon.com/
AWS Documentation: https://aws.amazon.com/documentation/
* Description
* Services
** VPC
Docs: https://aws.amazon.com/vpc/
** EC2
Docs: https://aws.amazon.com/ec2/
** ECS (EC2 Container Service)
Docs: https://aws.amazon.com/ecs/
** Lambda
Docs: https://aws.amazon.com/lambda/
** EBS
Docs: https://aws.amazon.com/ebs/
** EFS
Docs: https://aws.amazon.com/s3/
** S3
Docs: https://aws.amazon.com/rds/
** RDB 
Docs: https://aws.amazon.com/rds/
** Dynamo
Docs: https://aws.amazon.com/dynamodb/
** Redshift
** Elasticache
** ELB (Elastic Load Balancer)
Docs: https://aws.amazon.com/elasticloadbalancing/

- Classic Elastic LB
- Application Load Balancer
** Route53
Docs: https://aws.amazon.com/route53/
** CloudFormation
Docs: https://aws.amazon.com/cloudformation/
** Elastic Beanstalk
Docs: https://aws.amazon.com/elasticbeanstalk/
** SNS
Docs: https://aws.amazon.com/sns/
** SQS
** SWF
** API Gateway
** Cloudwatch
Docs: https://aws.amazon.com/cloudwatch/
** Cloudtrail
Docs: https://aws.amazon.com/cloudtrail/
** Kinesis
** EMR (Elastic MapReduce)
** AWSCLI
Docs: https://aws.amazon.com/cli/


*** Installing AWS CLI Tools
   
- makesure that pypi is installed (this may require the epel repo)
#+BEGIN_SRC sh
yum install python-pip
#+END_SRC

- install the aws cli
#+BEGIN_SRC sh
pip install awscli
#+END_SRC

*** User Authentication Configuration
    
- run the configuration cmd
#+BEGIN_SRC sh
aws configure
#+END_SRC
- this will ask for:
    - AWS Access Key ID [None]: 
    - AWS Secret Access Key [None]: 
    - Default region name [None]: 
    - Default output format [None]: 

- You will require the correct privileges to be assigned to your user in IAM

- Add both to your .bashrc: 
    - AWS Access Key ID [None]: 
    - AWS Secret Access Key [None]: 
#+BEGIN_EXAMPLE
export AWS_ACCESS_KEY_ID="add your key here"
export AWS_SECRET_ACCESS_KEY="add your secret here"
#+END_EXAMPLE

* Lecture
** [[https://www.youtube.com/watch?v%3DeKyS9rvbj40][Introduction to Database Services - Brian Rice]]
+ If you host your DB on-site
  - App optimization
  - Scaling
  - HA
  - DB bakups
  - DB patches
  - OS patches
  - OS installation
  - Server maintenance
  - Rack and stack
  - Power, HVAC, networking

+ Host DB in Amazon EC2
You:                     AWS:
  - App optimization       - OS installation
  - Scaling                - Server maintenance
  - HA                     - Rack and stack
  - DB bakups              - Power, HVAC, networking
  - DB patches
  - OS patches
    
+ Host DB with managed DB service
You:                        AWS:
  - App optimization           - Scaling
                               - HA
                               - DB bakups
                               - DB patches
                               - OS patches
                               - OS installation
                               - Server maintenance
                               - Rack and stack
                               - Power, HVAC, networking
				 
+ Managed Services
  - DynamoDB - NoSQL
  - RDS - SQL database engines
  - ElastiCache - In-memory cache
  - Redshift - Data warehouse
- These services provide alternatives to ssh to access the DB    

+ DynamoDB
  - Simple and fast to deploy
  - Simple and fast to scale
    - To millions of IOPS
  - Data is automatically replicated
  - Fast, predicatable performance
- No cost to get started; pay only for what you consume
  
+ Amazon RDS
  - Amazon Aurora, MySQL, PostgreSQL, Oracle, SQL Server
- No cost to get started; pay only for what you consume
- Choose a DB instance type with the right amount of CPU and memory
- Automated bakups
  - restore your db to a point in time
  - enabled by default
  - choose a retention period, up to 35 days
- Manual snapshots
  - persist until you delete them
  - stored in Amazon S3
  - Build a new DB instance from a snapshot when needed
- choose Multi-AZ
  - Availability Zone is a physically distinct independent infrastructure
  - Failover occurs automatically in response to the most important failure scenarios
  - failover usually under 90secs

+ ElastiCache
  - High performance, resizable in-memory caching
  - memcached and Redis engines
  - ElastiCache is wrapper around either of the above engine that is chosen
    
+ Redshift
  - Petabyte-scale columnar DB
  - Fast response time
    - ~ 10x that of typical relational stores
- Pricing $1,000 per TB per year
  - Uses PostgrsSQL JDBC/ODBC
  - is built to reduce I/O
    - data compression
    - zone maps
    - direct-attached storage

** [[https://www.youtube.com/watch?v%3DezpMM1dzN68][Using Domain Verification with Amazon Simple Email Service]]
Back in 2012 Free Tier
  - 2000 messages for free each day when you call SES from your EC2 instance or through AWS Elastic Beanstalk
    
SES - Getting Started
The Amazon SES Sandbox
 - verifying Senders(email accounts that are going to send mail)
 - in the sandbox 
   - 200 messages per 24 hours
 - in production
   - quota - starts at 10000 per day
   - still can only send from verified emails
** [[https://www.youtube.com/watch?v%3DVC0k-noNwOU][Amazon S3 Masterclass - Ian Massingham]]
- Secure
- Durable
- Highly-scalable object storage
- Store and retrieve

+ Use Cases
  - Backup & Archiving
  - Content Stroage & Distribution
  - Big Data Analytics
  - Static Website Hosting
  - Cloud-native Application Data
  - Disaster Recovery
    
*** Fundimental Concepts
  - Buckets
    - Containers for objects stored in S3
    - Consist of data & metadata
    - Combination of a bucket, key & version Id uniquely identify each object.
  - Regions
    - The geographical region where Amazon S3 will create your bucket
    - Will never leave that region, unless you move them
  - Web Store not a file system
  - APIs - http://aws.amazon.com/documentation/s3
  - SDKs - http://aws.amazon.com/tools/
  - Acces via AWS CLI - similar to bash cmds - ls, cp, mv, rm etc

+ Difference between fs and web store
  - write once, read many times (S3 reading more than writing)
  - Eventually consistent

+ Namespaces
  - Object key
    - Max 1024 bytes UTF-8
    - Unique within a bucket
    - Including 'path' prefixes
example - assets/js/jquery/plugins/jtables.js

+ Throughput Optimisation
  - S3 automatically partitions based upon key prefix
    
+ Access Controls
  - IAM Policies
    - fine grained control
    - Administer as part of role based access
    - Apply policies to S3 at role, user and group level
  - Bucket Policies - allow anonymous persons access to a bucket, a class etc
    - Fine grained
    - Apply policies at the bucket level in S3
    - Incorporate user restrictions without using IAM
  - ACLs
    - Coarse grained
    - Apply access control rules at the bucket and or object level in S3
*** Getting Started
- Class of storage
  - Standard - 99% durability and 99% availability
  - Reduced Redundancy Storage - reduced cost, but at lower levels of redundancy
  - Glacier - archiving data, where data access is infrequent and retrieval time of several hours is acceptable.
            - very low-cost
class can be specified on the aws cli 
#+BEGIN_SRC sh
aws s3 cp aws_uki.txt s3://aws-ianm-s3-masterclass/ --storage-class REDUCED_REDUNDANCY
#+END_SRC
class can also be changed in the AWS console(web interface)

- Encryption
  - Securing Data in Transit
    - SSL over HTTPS
    - Alternatively use a client encryption lib such as the Amazon S3 encryption client to encrypt your data before uploading to Amazon S3
      - this is done with a one time encryption key
  - Server Side Encryption (SSE) - 3 options
    - SSE-S3 key management - Amazon mgmt of keys
    - SSE-C - Customer-Provided Keys - Amazon disgards the key
    - AWS KMS (SSE-KMS) - this is a stand alone service
      - S3 with encrypt your data at rest using keys that you manage in the AWS key mgmt service (KMS)
      - KMS provides audit trail to see who used your key to access which object
	
- Audit logs
  - access logs can be created per bucket
    
- Multi-Factor Auth Delete

- Time-Limted Access to Objects
  - time limited urls to allow access to an object for a set time
    
- Versioning & Cross Region Replication
  - Bucket level
    - automatically preserves all copies of objects
  - Persistent
Versioning will increase costs, due to storing multiple copies of objects

- Lifecycle Rules
  - moving S3 buckets to glacier after a certain period of time
    - example would be transaction data after 30 days
  - deleting objects after a certain period of time
    - example would be logs after 30 days

- Website Hosting
  - you can host your entire static website on Amazon S3
* Tutorials
** AWS Foundations - CBT Nuggets
*** How to build a cloud presence
1. Going to the cloud: Traditional Method
Build your own cloud placing your equipment in a data center.
2. Going to the Cloud: AWS Method
Use AWS services to create your infrastructure.

**** Traditional Method
+ Setting up
  - Select a Data Center
  - Purchase Rack Space
  - Purchase Internet Connectivity
  - Install Equipment
    - Switches
    - Firewalls
    - Servers
    - Storage - SAN or NAS
  - Configure Services
  - Expand to More Data Centers - Locality is important when it comes to serequipmentvices such as VOIP
+ Pros & Cons:
  - Massive up-front cost, BIG "Steps"
  - IT Staff: focus on the data center 
  - In-House knowledge limits
  - recreate the wheel
  - It's yours
  - "Monster Server" Capabilities

**** AWS Method
+ Setting up
  - Pick your region
  - Pick your availability zone - these are physical data centers
    - for redundancy you should look at rolling out in to more than one zone
  - Provision your server
  - Configure services
  - Expand to other availability zones
  - Expand to other regions
+ Pros & Cons:
  - Pay As You Go; Pricing Models
  - Elastic Computing; Grow as needed
  - Economy of scale
  - Immediate security accreditation
  - Multiple data centers easily
  - Collaborative innovation
  - horizontal scaling

**** Vertical Vs Horizontal Scaling
***** Vertical Scaling 
  - Increasing HW
  - Increasing Capacity
  - Easy to do

***** Horizontal Scaling (scale out):
  - Increasing instances
  - Shared capacity
  - Typically requires planning

*** Getting Started with AWS
**** What you need to get started
- A Purpose
- Logon Information/Email Address
- A Credit Card/Phone number
***** An Understanding of the services
  - Cloudwatch 
    - Monitors all of the services
    - Can start to get expensive
  - EC2 - Elastic Compute Cloud
    - allows os templates to be created with specific functions db, web etc
    - public available timeplates
  - S3 - Simple Storage Service 
    - written to at least two places
    - Where your image is held whilst not being run
  - EBS - Elastic Block Store
    - faster than S3
    - optional to running image on the ephemeral memeory of the server
      - means that when the machine is shutdown it's data isn't lost
  - Route 53
    - create name records for your domains
    - manages dns
  - VPC - Virtual Private Cloud
    - site to site cloud
  - Auto Sacling
    - amazon automated server management tool
    - will spin up servers when certain limits are reached 
    - shutdown instances when website hits a lull
  - CloudFormation
  - IAM - Identity and Access Management
    - create credentials to access system
  - ELB - Elastic Load Balancing
  - SimpleDB/DynamoDB
    - simpleDB now discontinued
    - Dynamo is really fast
***** AWS Management Console
*** Creating an EC2 Instance - AMI Selection (Amazon Machine Image)
+ Considerations for Provisoning Instances
- In a region all availability zones are connected by high speed fiber.
- Between reigons you are running over the internet and this becomes the dependent factor for data transfer and you should be aware.
- AMI can come with software installed, LAMP, SQL Server etc
- Customized AMIs are stored in S3, this is charged.
- How many instances ?
- Instance type? - micro is available on the Free Tier
- AWS Market Place sells AMIs from different providers offering different software.

*** Understanding EC2 Pricing Models
**** On-demand Instance Pricing
- no commit model
- costs a little more due to this fact
- pricing fluctuates with region
**** Reserved Instance Pricing    
- 1 or 3 year term contract that will lower the rate paid/hour on instances
- Types - these are nothing to do with box performance
    - Light
    - Medium
    - Heavy
The difference in these types is the costing, light you pay less up front but your hourly rate is higher, and Heavy is the opposite, more up front but less per hour.
**** Spot Instance Pricing
- Bidding on left over CPU memory that the data center has available
- But if out bid you lose your resources are shutdown
- The more requirements adds to the chance that you will lose your instance if it is accepted at all.
*** Understanding EC2 Instance Types
**** Measureing Instance Types
+ Instance Types always include a mix of:
  - Memory
  - Processing Power
  - Storage
  - I/O performance
    
+ Instance Families
  - Micro
  - Small
  - Medium
  - Large
  - Extra Large

+ Specific cases
  - High Memory
  - High CPU
  - Cluster

+ Amazon Best Practice: Start small, benchmark and scale up in necessary

**** Understanding Processing Power Ratings
- Everything in AWS is "Virtual" but there really are physical items!
- To provide consistant performance, created the EC2 Compute Unit (ECU)
  - is equivalent to a 1.0 to 1.2 GHz 2007 Xeon Processor
  - it is then split over the number of cores specified by the type
    
**** Understanding I/O Rating
- I/O ratings measure shared resources(Network/Disk)
- Equal shares given to the instances
- I/O Levels
  - Low
  - Moderate
  - High
  - Very High
- Heavy disk performance can benefit from a RAID 0 set across 4 disks
  - obviously risk that comes with RAID 0 one failed disk all gone!!
*** Understanding Tags and Key Pairs
+ Tags
- Tags are a way to identify instances    
  - develop a logical naming convention
- These tags appear on the instance dashboard
- Show/Hide button allows you to customize which tags are visable.
  
+ Key Pairs
- These are the pub/priv key pair that are issued by AWS.
- Windows Key Pair
  - this key gives you the default windows password
  - you get this by right clicking on the window instance and click on "Get Windows Password"
  - you will then be challenged for the priv key to unlock the encrypted password.
- Linux Key Pair
  - this is how you will connect to the machine unless you change the key 
    
+ What if I lose my key?
- Amazon has no way for you to get your priv key again.
- If you have an instance that you need to access, you will need to create an AMI of that instance and recreate it. All of your data will be there but it my require some admin, such as remounting of disks etc.

*** Understanding Security Groups
**** Security Groups: Your EC2 Firewall
- Inbound filtering for your instances
- "Security Groups" - can be individual (Group of one) or multiple
- By default - 
  - Rules: No traffic inbound, all traffic outbound, all traffic within group
- Changing security groups can only be done inside VPC
- Good practice to split DB and Web servers into different secuirty groups
- Don't open RDP(3389) to the world lock it down to your ip, like you do with linux ssh.

*** Understanding Elastic IPs and ELB

*** SES, SNS, SQS
SES - Simple Email Service
  - AWS service allowing you to send email from hosted applications
  - Designed for bulk service
  - Leverages AWS email reputation, volume
  - Outbound scanning on all email sent
  - Uses AWS closed-loop system
  - Accounts limited to 10,000 emails/day, quantity automatically increases
  - Charged based on quantity of email sent
    
SNS - Simple Notification Service
  - Message transmission for humans and services
  - Protocols: HTTP/HTTPS, Email, SMS, SQS
  - SNS Topic created, subscribers added, AWS services report to a topic
  - As with everythin, pay-as-you-go... first million API requests/month free

SQS - Simple Queue Service
  - Message Queuing System
  - Allows you to build applcations without concerns of how communication is stored or handled
  - Unlimited messages, unlimited queue size
  - Message payload up to 25KB 
  - $0.50 / million SQS requests

** AWS Concepts - Linux Academy
[[file://home/crito/Documents/SysAdmin/Cloud/AWS/aws-concepts-pps.pdf][AWS Concepts PPS]]

** AWS Essentials - Linux Academy
http://bit.ly/2guw5giiiY
** AWS Certified SysOps Administrator
[[file://home/crito/Documents/SysAdmin/Cloud/AWS/sysops/AWS_Auditing_Security_Checklist.pdf][AWS Auditing Security Checklist]]
[[file://home/crito/Documents/SysAdmin/Cloud/AWS/sysops/AWS_Backup_Recovery.pdf][AWS Backup Recovery]]
[[file://home/crito/Documents/SysAdmin/Cloud/AWS/sysops/AWS_Building_Fault_Tolerant_Applications.pdf][AWS Building Fault Tolerant Applications]]
[[file://home/crito/Documents/SysAdmin/Cloud/AWS/sysops/AWS_certified_sysops_associate_blueprint.pdf][AWS Certified SysOps Associate Blueprint]]
[[file://home/crito/Documents/SysAdmin/Cloud/AWS/sysops/AWS_Cloud_Architectures.pdf][AWS Cloud Architectures]]
[[file://home/crito/Documents/SysAdmin/Cloud/AWS/sysops/AWS_Disaster_Recovery.pdf][AWS Disaster Recovery]]

*** Lesson 3 - Understanding AWS Instance Types, Utilization and Performance
- Virtualization Types
  - HVM AMIs (Hardware Virtual Machine)
    - Can use special hardware extensions
    - Can use PV drivers for network and storage
    - Usually the same or better performance than PV alone

  - PV AMIs (Paravirtual)
    - Historically faster than HVM, but no longer the case
    
- Instance Types
  - General Prupose
    - T2
      - intended for work loads that do not use the full CPU often or consistently
      - Provided Burstable Performance
    - M3
      - Provide a balance of compute, memory and network resources
      - SSD Storage (Instance store)
    - M4
      - Provide a balance of compute, memory and network resources
      - Support Enhanced Networking
      - EBS-optimized (doesn't allow ssd storage)
	
  - Compute Optimized
    - Lowest price/compute performance in EC2
    - C3
      - SSD-backed instance storage
      - Support for enhanced networking and clustering
    - C4
      - Latest generation of compute-optimized instances
      - hightst performing processors (optimized specifically for EC2)
      - support for enhanced networking and clustering
      - EBS-optimized
	
  - Memory Optimized (big data, such as spark)
    - Lowest price per amount(GiB of RAM) and memory performance
    - R3
      - SSD-backed instance storage
      - High memory capacity
      - Support for enhanced networking
	
  - GPU 
    - Graphics and general purpose GPU compute
    - G2
      - High frequency processors
      - high-performance NVIDA GPUs
      - On-board hardware video encoder
      - Low-latency frame capture and encoding, enabling interactive streaming
      - Useful for GPU compute workloads, machine learning, video encoding 3D application streaming, etc...
	
  - Storage Optimized (Hadoop, data warehousing, MongoDB)
    - Very fast SSD-backed instance storage optimized for high random I/O performance and high IOPS
    - I2
      - high I/O performance
      - high frequency processors
      - ssd storage
      - supports TRIM (free up space)
      - supports enchanced networking
	
- Burstable Performance
  - cpu credits are used to burst past the baseline performance up to 100% of a cpu core
  - credits are gained every hour
  - aws provides an initial amount to ensure that the cpu isn't struggling at start up

*** Lesson 4 - EC2 Instance and System Status Checks

- System Status Checks
  - Loss of network connectivity
  - Loss of system power
  - Software issues on the physical host
  - Hardware issues on the physical host
    
  - Solutions
    - Stop and start instances
    - Terminate and re-launch instances
    - Contact AWS
      
- Instance Status Checks
  - Failed system status checks
  - Incorrect networking or startup config
  - Exhausted Memory
  - Corrupted file system
  - Incompatible kernel
    
  - Solutions
    - Solve what is causing the issue
    - Stop and start instances
    - Terminate and re-launch instances with more memory, a different kernel, or different networking config
      
*** Lesson 5 - CloudWatch Alarms

Alarm state
  - OK           - is within defined thershold
  - ALARM        - is outside of thershold
  - INSUFFICIENT - alarm has just been started, or has insuffiecient data to accurately report
    
- CloudWatch doesn't have metrics for memory, this requires scripts to be provided on the instance
  
- Under Rules you can create cron jobs
  
*** Lesson 6 - Installing and Configuring Monitoring Scripts for EC2 instances

- The scripts will require the permissions to access CloudWatch
  
- CloudWatch will report information at 5min intervals for more detailed reporting you need to enable detailed monitoring.
  - Detailed monitoring is a chargable service (reports every one minute)
    
- install perl, get the monitoring scritps, unzip and run the mon-put-instance-data.pl script
#+BEGIN_SRC sh
sudo yum install perl-Switch perl-DateTime perl-Sys-Syslog perl-LWP-Protocol-https
curl http://aws-cloudwatch.s3.amazonaws.com/downloads/CloudWatchMonitoringScripts-1.2.1.zip -O
unzip CloudWatchMonitoringScripts-1.2.1.zip 
./mon-put-instance-data.pl --mem-util --mem-used --mem-avail --swap-util --swap-used --disk-space-util --disk-space-used --disk-space-avail --memory-units=megabytes --disk-space-units=gigabytes --disk-path=/dev/xvda1
#+END_SRC
- A mon-get-instance-stats.pl is also provided, this script allows us to pull data
  
- also set the mon-put-instance-data.pl to a cron job
#+BEGIN_EXAMPLE
*/5 * * * * ~/aws-scripts/mon-put-instance-data.pl --mem-util --mem-used --mem-avail --swap-util --swap-used --disk-space-util --disk-space-used --disk-space-avail --memory-units=megabytes --disk-space-units=gigabytes --disk-path=/dev/xvda1
#+END_EXAMPLE
these metrics will now be able to be viewed on the dashboard under linux metricsd

*** Lesson 6 - Dedicating an Instance to Monitoring
*** Lesson 7 - Monitoring EBS for Performance and Availability
    
- EBS uses IOPS (I/O operations per second) as a performance measure
- IOPS measured in 256 KiB (Kibibytes) chunks of I/O operations for SSDs
  - SSDs deliver constant preformance for both random and sequential I/O operations
  - 4000 IOPS can transfer 4000 256KiB chunks per second
  - 5 I/O operations at 54KiB will count as 5 operations
- IOPS measured in 1024 KiB chunks of I/O operations for HDDs
  - HDDs have optimal performance with large and sequential I/O operations
  - 8 sequential 128KiB operations will count as 1 operation
  - 8 random 128KiB operations will count as 8 operations
    
- SSD-backed volumes
  - Two different types of SSD volumes: io1 and gp2
  
  - gp2 - General Purpose(default)
    - Baseline performance of 3 IOPS per GB up to 10,000 IOPS
    - Minimum of 100 IOPS (ie: 8 GB volume has 100 IOPS instead of 24)
    - The larger the volume, the more IOPS
    - Can burst up to 3000 IOPS if the size is under 1TB
    - up to 160 MiB/s of throughput
      
  - volumes get credits at the 3 IOPS per GiB of volume size per second
    - volumes start out with their maximum amount of 5.4 million I/O credits
    - running out of credits causes the volume to revert back to baseline IOPS performance
      
  - io1 - Provisioned IOPS
    - ideal for IOPS-intensive and troughput intensive workloads (like db)
    - Baseline prformance of 30 IOPS per GB up to 20,000 IOPS
    - Does not use credits to burst above baseline performance, instead it gives a consistent IOPS rate
    - Delivers within 10 percent of provisioned IOPS performance 99.9. percent of the time in a given year
    - up to 320 MiB/s of throughput
      
- HDD-backed volumes
  - Throughput Optimized HDD (st1 and Cold HDD (sc1)
    - can sometimes provide more throughput (MB/s) but drastically less IOPS

  - Throughput Optimized HDD - st1	
    - ideal for frequently accessed and throughput intensive workloads

  - Cold HDD - sc1 
    - less frequently accessed workloads
    - lowest cost HDD volume
      
- Performance - Pre-warming/initialization
  - initialisation is no longer needed for new EBS volumes
    - EBS volumes get maximum performance right away
    - Storage blocks on volumes restored from snapshots do need to be initialized
      
  - initialisation can be accomplished by reading from all blocks on a volume with dd or fio utilities
  #+BEGIN_SRC sh
  sudo dd if=/dev/xvdf of/dev/null bs=1M
  #+END_SRC
  
- GetMetricStatistics
  - Volume ReadBytes & VolumeWriteBytes
    - The sum statistic reports the total number of bytest transferred
    - Average is also useful to see the average size of each I/O operation
  - VolumeReadOps & VolumeWriteOps
    - Represents the total number of I/O operations
    - You can calculate the average I/O operations per second (IOPS) for a period by dividing the total operations by the number of seconds in that period
  - VolumeTotalTime & VolumeTotalWriteTime
    - The total number of seconds spent by all operations in a given time period
    - A steady increase in these numbers could indicate the need to increase volume size or increase the number of provisioned IOPS
  - VolumeQueueLength
    - Number of read/write operations requests waiting to finish
      
- Provisioned IOPS Metrics
  - VolumeThroughputPercentage
    - The percentage of I/O operations per second that we achieved out of the total perovisioned IOPS for our EBS volume
  - VolumeConsumedReadWriteOps
    - The total amount of read and write operations consumed within a specific time period
      
- EBS Status Checks
  - status checks run every 5 minutes to determine the status of a volume
    - if all checks pass, the status is ok
    - if a check fils, the status is impaired
    - if the checks are running,the status is insufficient-data
      
  - When Amazon EBS finds that data might be inconsistent on a volume it disables I/O to that volume (by default)
    - This helps prevent data corruption
    - It causes a volume status to be impaired which can alert you

*** Lesson 8 - Monitoring RDS for Performance and Availability

- RDS - Monitoring Metrics	
  - CPUUtilization                 - Percentage of CPU utilization
  - DatabaseConnections            - Number of connections that we have at a given point in time
  - DiskQueueDepth                 - Number of read/write requests waiting to access the disk
  - FreeableMemory                 - Amount of available RAM
  - FreeStorageSpace               - Amount of available storage space
  - SwapUsage                      - Increase in this usually has to do with running out of available RAM   
  - ReadIOPS/WriteIOPS             - If not enough IOPS, performance will slow down
  - ReadLatency/WriteLatency       - Higher latency can be solved with more IOPS
  - ReadThroughput/WriteThroughput - Average number of bytes read or written to or from disk per second
    
*** Lesson 9 - Monitoring ElastiCache for Performance and Availability (caching)
    
- ElastiCache supports two engines
  - Memcached
  - Redis
    
- Monitoring Metrics
  - CPU Utilization
  - Evictions
  - CurrConnections
  - Swap Usage (Memcached)
    
- CPU Utilization
  - Memched is multi-threaded
  - Redis is single-threaded
    
  - Memcached
    - Can handle loads of up to 90%
    - Above 90% becomes a problem
    - Solution - vertical or horizontal scaling
      
  - Redis
    - Calculate the threshold: 90/# of CPU cores
    - Solution:
      - For read-heavy workloads, increase the number of read replicas
      - For write-heavy workloads, use a larger cache instance
	
- Evictions
  - Evictions happen when a new item is added but there is no more space. An older item must be deleted to make space.
  - Evictions can be a caching technique used to make sure you don't run out of memory
  - If an items getting evicted too frequently, it defeats the purpose and will decrease performance
  - CloudWatch alarms can notify you of a certain threshold
    
  - Memcahed solution - Increase instance size or add nodes to your cluster
  
  - Redis solution - Increase the node size
    
- Current Connections
  - An increase in CurrConnections could indicate a larger problem with your application
    - The app may not be releasing connections
    - Choose a threshold based off of your application requirements
      
- Swap usage (Memcached)
  - swap usage should stay at 0, and not exceed 50MB
  - Swap affects performance and should be avoided
    
  - Solution
    - increase node size
    - increase out ConnectionOverhead parameter value
      
*** Lesson 10 - Monitoring the Elastic Load Balancer for Perdformance and Availability

- Monitoring Metrics
  - Latency 
    - time it takes to receive a response  
    - measure the AVG and MAX values to spot abnormal activity
      
  - BackendConnectionErrors
    - Number of connections that were not successfully established between our load balancer and registered instances
    - Measure SUM and use the different between the minimum and maximums to spot issues
      
  - SurgeQueueLength
    - Measures the total number of requests that are waiting to be routed by the LB
    - Queue can hold a total of 1024 requests
    - Measure the MAX to see the peak of queued requests
    - AVG can also be used with MIN and MAX to get a range
      
  - SpilloverCount
    - if the SurgeQueueLength is full, requests "spill over" and get dropped
    - Measure the SUM
      
  - Pre-warming
    - if you are expecting a sudden and very large increase in traffic, you need to pre-warm your ELB to avoid dropped requests
      
*** Lesson 11 - AWS Billing and Linking AWS Accounts
*** Lesson 12 - AWS Billing Dimensions and Metrics for CloudWatch    
- Once Recieve Billing Alerts is activated it cannot be un-activated
*** Lesson 13 - Cost Optimizing
    
- Save costs by purchasing reserved instances
  
- Reserve instances for 1 to 3 yrs at a discounted rate
  - pay all, in part, or nothing upfront
  - the more you pay upfront, the more you save
    
- Low Utilization
  - save costs by minimizing the number of EC2 instances in-use
  - set ClouldWatch alarms to spin down underutilized instances
    - Example: 5% CPU utilization for 50 minutes
      
  - Find the right balance between availability and cost
  
  - remove unused LB as these are charges per LB
    
  - EBS volumes cost, enven when not in-use
    - delete unused volumes
    - take a snapshot if you want to keep the data
  
  - Provisioned IOPS cost more, make sure you're not provisioning more than necessary
    
  - Downsize volumes that have non-required space
    
  - EIPs cost money, if not in use disassociate them
   
*** Lesson 14 - Using the AWS Price List API and Cost Explorer
*** Lesson 15 - Scalability and Elasticity Essentials    

- What is elasticity?
  - the ability to scale up for demand, then retract back when demand slows down
  - pay only for what yoy need, when you need it
    
- Scalability Fundamentals
  - Scalabiliity focuses meore on building for growth
  - Examples:
    - Increasing instance size
    - Increasing the number of available instances
    - Increasing vol capacity
      
- DynamoDB
  - Scalability
    - we can keep storing more and more data without having to provision any hardware
      
  - Elasticity
    - We can increase or decrease read and write throughput capacity on demand
    - As read requests increase, we can increase read throughput capacity
    - As read requests slow down, we can decrease capacity
      
- EC2
  - Scalability
    - we can increase the size of the instance
    - there are different instance types we can choose from to grow 
    - launch more instances
      
  - Elasticity
    - auto scaling gives the ability to grow with demand, and shrink back during slower periods
      
- RDS
  - Scalabiliity
    - we can increase the size of instaces
    - launch read replicas
    - there are different instance tyeps we can choose from to grow
      
  - Elasticity
    - limited

*** Lesson 16 - Determing Reserved Instance Purchases Based on Business Needs
    
- Reserved Instances
  - Reserved instances give us the ability to purchase instance capacity for a specific period time
  - We can choose standard reserved instances or scheduled reserved
  - Offers discounts
  - Reserves capacity
    
*** Lesson 17 - AutoScaling vs Resizing
    
- Autoscaling
  - distributes the load across multiple instances
  - uses metrics and rules to automate spinning up/terminating instances
    
- Changin instance sizes
  - increases/decreases resources available to our application
    
- When to choose one over the over?
  - they both have pros and cons
    
- Think about if a EC2 Compute Optimized may be more appropriate for the instance type
  
- Scheduled Scaling
  - Auto scaling can scale or shrink on a schedule
    - one time occurrence or recurring schedule
    - can define a new minimum, maximum and scaling size
    - lets you scale out before you actually need capacity in order to avoid delays
      
- Challenges of Auto Scaling
  - relatively complicated to setup
    - instances can be started and stopped at any time
    - applicatiions need to be designed to handle distributed work
    - Important data (sessions, images, etc...) needs to be stored in a central location
    - If one server terminates, the application should still function
  - Delays in scaling
    - Instances take time to initialize
    - Applicatins may require setup which could take even more time

- Challenges of Resizing Instances      
  - Compatibility
    - instances must have the same virt type to resize
    - incompatible instances require migration
  - EBS- backed instances need to be stopped before resize
  - Instance store-backed instances require migration by creating an imamge and launching a new instance from the image
  - Resizing isn't very flexible comparted to Auto scaling
  - There usually has to be downtime and careful planning
  - Resizing instances in Auto Scaling groups may need "suspending"
    
*** Lesson 18 - Elastic Load Balancer Sticky Sessions
    
- Though cookies can be issued with the LB and instances behind, but this may lead to unevenly distributed traffic and ineffect the LB being bypassed due to the cookies
  
- Elasticache is the prefered method, where the session data could be saved in RDS. This would ensure that the traffic is evenly distributed by the LB.
      
*** Lesson 19 - High Availability with Single Instance Applications that Require Elastic IP Addresses
    
- Problem - older applications moved to AWS might require static IP addresses
  - reasons for this generally include IP addresses hard coded into the code
  - Would require serious commitment to change it
    
- How can you make an application like this highly available and fault tolerant?
  - use an elastic ip (EIP)
  - Understand why Auto scaling will not work
  - create a standby instance in other availability zones
  - increase instance size to scale
    
*** Lesson 20 - Understanding RDS Multi-AZ Failover
    
- RDS Multi-AZ Failover
  - Provisions and maintians a standby replica in a different AZ  
  - The primary synchronously replicates to the standby instance for redundancy
  - Can reduce downtime in the event of a failure on the Primary
    
- How does replication work?
  - The feature can be turned on from the console or API
  - Amazon automatically handles replication
  - The primary instance synchronously replicates to the standby instance for redundancy
  - Replication can cause higher write and commit latency
    - using provisioned IOPS is recommended
      
- Other benefites of replication
  - Patching
    - patching can be done on the standby instance first, and the on the primary to minimize downtime
  - Backups
    - we can eliminate I/O locking and minimize latency spikes
    - create backups from the standby instance
      
- What can trigger a failover?
  - loss of availability in the primary availiability zone
  - loss of network connectivity to the primary instance 
  - resource failure with the underlying virtualized resources
  - storage failure on the primary database
  - the db instance's server type is changed
  - software/OS patching
  - a manual reboot with failover was initiated

- How do failovers work?
  - The Process is automated by AWS
    1. Amazon detects an issue and starts the failover process
    2. DNS records are modified to point to the standby instance
    3. The application re-establishes any existing DB connections
       
*** Lesson 20 - Applying High Availability Bastion Host Instance
    
- Bastion Hosts
  - "Gate" that protects our infrastructure but allows access for updates or other management
  - Used to control remote access (e.g. via RDP or SSH)
  - For inbound traffic exposed to the internet
  - These should be hardened and secured very carefully and reularly updated
    
- Other Benefits
  - can have an Elastic IP Address that never changs and can be whitelisted
  - we can have standby Bastion Hosts for higher availability
    
*** Lesson 21 - Overview of Services that Allow Access to the Underlying Operating System
    
- EMR - Elastic MapReduce
- EC2 - Elastic Cloud Compute
- ECS - Elastic Container Service
- Elastic Beanstalk 
- OpsWorks - Configuration management
  
- Services that don't allow access to the underlying OS
  - RDS
  - DynamoDB
  
*** Lesson 22 - Elastic Load Balancer Configuration
    
- we can have both external and internal LB
  
- External LB are public facing
  - often used to distribute load between web servers
  - provides public DNS hostname
    
- internal load balancer are not customer facing
  - often used to distribure load between private backend servers
  - provides an internal DNS hostname
    
*** Lesson 23 - Offloading Database Workload
    
- RDS Read Replication
  - Read replicas can be used to offlaod work from the main db  
    - writes go to the source instance
    - reads go to the read replicas
      
- Create the read replica    
  - select the source db
  - a snapshot is taken and is applied to the instance that is to become the read replica
      
- RDS Read Replication vs Mutli-AZ failover
  - read replicas are built primarily for performance and offloading work
  - Multi-AZ deployments are used for high availability and durability
  - Multi-AZ deployments give us synchronous replication instaead of asynchronous
  - Multi-AZ deployments are only used to perform a failover, they are idle the rest of the time
  - Read replivas are used to serve legitimate traffic
  - It is often beneficial to use both of these as complements

- which engines to support read replicas
  - innodb
  - extradb
    
  - myisam causes problems, better to use innodb
  
- automated backups has to be initialized for read-replicas to be created
  
*** Lesson 24 - Initializing (Pre-warming)EBS Volumes
*** Lesson 25 - Pre-Warming the Elastic Load Balancer
    
- HTTP 503 Error (ELB cannot handle anymore requests)
  - does not queue requests but instad drops them
    
- ELB dis designed to increase its resource capacity with gradual increases in traffic
  
- When expecting significant spikes in traffic it is possible the traffic is sent faster than the ELB can "expand"
  - contact aws for "pre-warming" of the ELB
    
*** Lesson 26 - Resizing or Changing EBS Root Volume

1 - create a snap shot of the current root volume
2 - with this snap shot choose to create a volume from it
3 - setting a larger size volume will increase the number IOPs available
4 - stop the instance that the new volume is to be attached too
5 - attach the new volume to the stoped instance
6 - restart the instance
7 - ssh into the instance and check that the volume is mounted correctly
    - lsblk or df
    - if full volume not seen use the resize2fs cmd
      
*** Lesson 27 - SSL on Elastic Load Balancer

IAM - should be used if the certs are from a 3rd party
ACM - should be used if the certs are from amazon

*** Lesson 28 - Network Bottlenecks
    
- Potential Issues
  - One of the primary network bottlenecks comes from EC2 instances
  - Instance are in different Availability Zones, regions or continents
  - EC2 instance sizes (larger instances generally have better bandwidth performance)
  - not using enhanced networking features

- performance can be checked with iperf3
  
- VPCs can use VPC peering to create a reliable connection
  - no single point of failure for communication or bandwidth bottlenecks
    
- using iperf3 to monitor/bench mark networking
#+BEGIN_SRC sh
iperf3 -s -p 80
#+END_SRC
p - 80
- on another instance install iperf3 (ubuntu instance in this case), if not available in repo of distro it is available from github
#+BEGIN_SRC sh
apt-get install iperf3
#+END_SRC
- on this instance connect back to the instance we are testing
#+BEGIN_SRC sh
iperf3 -c 53.234.170.10 -i 1 -t 10 -p 80
#+END_SRC
c - connect
i - interval
t - duration of time
This will provide detailed information of each interval and an overall sender/reciever bandwidth

- Bandwidth limitations on your VPN to your AWS VPC
  - Using VPN to access AWS VPC from our on-premise network means we have to communicate over the open internet
    
- We can use AWS Direct Connect
  - Gives us a dedicated network connection
  - sets up a private connection
  - can reducee costs in some situations 
  - supports post speeds of 1Gbps and 10Gbps
  - Speeds of 50Mbps, 100Mbps, 200Mbps, 300Mbps, 400Mbps and 500Mbps can be ordered through an APN Partner supporting AWS Direct Connect

*** Lesson 29 - Lab - Test Bandwidth on EC2 instances with iperf3
*** Lesson 30 - EBS Root Devices on Terminated Instances - Ensuring Data Durablility
    
- delete on termination is set a default
  - for persistance this should be unticked
    
- backing up data
  - uncheck the delete on termination
  - create a snapshot before you terminate the instance
  - create a volume to backup other volumes too.   

*** Lesson 31 - Troubleshooting Auto Scaling Issues
    
- Attempting to use the wrong subnet
- Availability is no longer available or supported
- Security group does not exist
- Key pair associated does not exist
- Auto scaling configuration is not working correctly
- Instance type specification is not supported in that Availability Zone
- Auto Scaling service is not enabled on the account
- Invaild EBS device mapping
- Attempting to attach EBS block device to instace-store AMI
- AMI issues
- Placement group attempting to use m1.large (wrong instance type)
- "We currently do not have sufficient instance capacity in the AZ that you requested"
- Updating instance in Auto Scaling group with "suspended state" 
  
*** Lesson 32 - OpsWorks: Overview
    
- What is OpsWorks
  - give us flexible way to create and manage resources for our applications, as well as the applications themselves.
  - we can create a stack of resources and manage those resources collectively in different layers. These layers can have built-in or suctom Chef recipes.    

    - Overall
      - automate deployments
      - monitor deployments
      - maintain deployments

  - Anatomy
    - Stacks
      - represent a set of resources that we want to manage as a group
        - e.g EC2 instances, EBS volumes, LB
      - We could build a stack for a development, staging or production environment
	
    - Layers
      - Used to represent and configure components of a stack
        - e.g. a layer web app servers, a layer for the db, and a layer for the LB
      - we can use built-in layers and customize those or create completley custom layers
      - recipes are added to layers
	
    - Instances
      - must be associated with at least one layer
      - we could build a stack for a development, staging, or production environment
      - we can run as:
        - 24/7
        - load-based
        - time-based

    - Apps
      - Apps are deployed to the application layer through a source code repo likt Git, SVN or seven S3.
      - We can deploy an app against a layer and have ops works exec recipes to prepare instancees for the app.
	
Layer  -----  LB

Layer  -----  Instances

Layer  -----  DB Instance

  - Recipes
    - created using the ruby language and based off of the chef deployment software
    - custom recipes can customize different layers in an application
    - recipes are run at certain per-defined events within a stack
      - Setup - occurs on a new instance after first boot
      - Configure - occurs on all stack instances when they enter or leave the online state
      - Deploy - occurs when we deploy an app
      - Undeploy - happens when we delete an app from a set of application instances
      - Shutdown - happens when we shut down an instance (but before it is actually stopped)

*** Lesson 33 - OpsWorks: Creating our First Stack
*** Lesson 34 - CloudFormation Essentials
    
- CloudFormation allows you to create and provision resources in a reusable template fashion
- turns your resources into stacks that work as units
- allows you to source control your infrastructure
- templates are JSON compatible
  
- Version and Description
  - AWSTemplateFormatVersion
    - Specifies which template version you want to use
  - Description
    - This section follow the template version section
    - Descriptions help clearly differentiate between templates
  - Metadata
    - JSON objects that provide details about the template
  - Parameters
    - Valuees you can pass in right before template creation
    - allows you to customize templates
    - can have default values as well as allowed values
  - Mappings
    - Lets you map keys to values
    - for example: you can make different valuees for different regions
  - Conditions
    - Can check values before deciding what to do
    - Allows you to create different resources in the same template depending on the condition evaluation
    - Example: can create different environments for development and production
  - Resources (required)
    - this is where you create different resources
  - Outputs
    - can ouptu values that you'd like see from the console of from API calls
      
  - Intrinsic Functions
    - used to pass in values that are not avaklable until run time
    - Fn::GetAtt 
    - Fn::FindInMap - redturns the value of a key from a specified mapping
    - Fn::Join - Concat elements, separated by a specified delimiter
    - Ref - Returns a resource or value based on a logical name or parameter
    - Fn::GetAZs - Get the AZ for a specified region
    - Fn::Select - Returns a single object from a list of objects by index
    
  - CloudFormation Rollback
    - if a stack fails to create a resource, by default the stack will rollback
    - Rollback - Removal of all created resources after a failed stack creation, or after cancelling creation
    - Rollback can be disabled from the API
      
  - Advanced Concepts
    - templates allow you to declare cloud-init scripts for EC2 resources
    - templates allow the use of regex in certain declarations

*** Lab - CloudFormation
Lab Guide: [[file://home/crito/Documents/SysAdmin/Cloud/AWS/LA_Lab_Guide_CloudFormation.pdf][Linux Academy Lab Guide for CloudFormation]]
#+BEGIN_EXAMPLE
{
  "AWSTemplateFormatVersion" : "2010-09-09",

  "Description" : "Introduction to CloudFormation",

  "Mappings" : {	

    "SubnetConfig" : {
      "VPC"     : { "CIDR" : "10.0.0.0/16" },
      "Public"  : { "CIDR" : "10.0.0.0/24" }
    }
  },

  "Resources" : {

    "VPC" : {
      "Type" : "AWS::EC2::VPC",
      "Properties" : {
        "EnableDnsSupport" : "true",
        "EnableDnsHostnames" : "true",
        "CidrBlock" : { "Fn::FindInMap" : [ "SubnetConfig", "VPC", "CIDR" ]},
        "Tags" : [
          { "Key" : "Application", "Value" : { "Ref" : "AWS::StackName" } },
          { "Key" : "Name", "Value" : "LinuxAcademy" },
          { "Key" : "Network", "Value" : "Public" }
        ]
      }
    },
    "PublicSubnet" : {
      "Type" : "AWS::EC2::Subnet",
      "Properties" : {
        "VpcId" : { "Ref" : "VPC" },
        "AvailabilityZone": { "Fn::Select": [ "0", { "Fn::GetAZs": "" } ] },
        "CidrBlock" : { "Fn::FindInMap" : [ "SubnetConfig", "Public", "CIDR" ]},
        "Tags" : [
          { "Key" : "Application", "Value" : { "Ref" : "AWS::StackName" } },
          { "Key" : "Network", "Value" : "Public" }
        ]
      }
    },
    "InternetGateway" : {
      "Type" : "AWS::EC2::InternetGateway",
      "Properties" : {
        "Tags" : [
          { "Key" : "Application", "Value" : { "Ref" : "AWS::StackName" } },
          { "Key" : "Network", "Value" : "Public" }
        ]
      }
    },
    "GatewayToInternet" : {
      "Type" : "AWS::EC2::VPCGatewayAttachment",
      "Properties" : {
        "VpcId" : { "Ref" : "VPC" },
        "InternetGatewayId" : { "Ref" : "InternetGateway" }
      }
    },
    "PublicRouteTable" : {
      "Type" : "AWS::EC2::RouteTable",
      "Properties" : {
        "VpcId" : { "Ref" : "VPC" },
        "Tags" : [
          { "Key" : "Application", "Value" : { "Ref" : "AWS::StackName" } },
          { "Key" : "Network", "Value" : "Public" }
        ]
      }
    },
    "PublicRoute" : {
      "Type" : "AWS::EC2::Route",
      "DependsOn" : "GatewayToInternet",
      "Properties" : {
        "RouteTableId" : { "Ref" : "PublicRouteTable" },
        "DestinationCidrBlock" : "0.0.0.0/0",
        "GatewayId" : { "Ref" : "InternetGateway" }
      }
    },
    "PublicSubnetRouteTableAssociation" : {
      "Type" : "AWS::EC2::SubnetRouteTableAssociation",
      "Properties" : {
        "SubnetId" : { "Ref" : "PublicSubnet" },
        "RouteTableId" : { "Ref" : "PublicRouteTable" }
      }
    },
    "PublicNetworkAcl" : {
      "Type" : "AWS::EC2::NetworkAcl",
      "Properties" : {
        "VpcId" : { "Ref" : "VPC" },
        "Tags" : [
          { "Key" : "Application", "Value" : { "Ref" : "AWS::StackName" } },
          { "Key" : "Network", "Value" : "Public" }
        ]
      }
    },
    "InboundHTTPPublicNetworkAclEntry" : {
      "Type" : "AWS::EC2::NetworkAclEntry",
      "Properties" : {
        "NetworkAclId" : { "Ref" : "PublicNetworkAcl" },
        "RuleNumber" : "100",
        "Protocol" : "6",
        "RuleAction" : "allow",
        "Egress" : "false",
        "CidrBlock" : "0.0.0.0/0",
        "PortRange" : { "From" : "80", "To" : "80" }
      }
    },
    "InboundSSHPublicNetworkAclEntry" : {
      "Type" : "AWS::EC2::NetworkAclEntry",
      "Properties" : {
        "NetworkAclId" : { "Ref" : "PublicNetworkAcl" },
        "RuleNumber" : "102",
        "Protocol" : "6",
        "RuleAction" : "allow",
        "Egress" : "false",
        "CidrBlock" : "0.0.0.0/0",
        "PortRange" : { "From" : "22", "To" : "22" }
      }
    },
    "OutboundPublicNetworkAclEntry" : {
      "Type" : "AWS::EC2::NetworkAclEntry",
      "Properties" : {
        "NetworkAclId" : { "Ref" : "PublicNetworkAcl" },
        "RuleNumber" : "100",
        "Protocol" : "6",
        "RuleAction" : "allow",
        "Egress" : "true",
        "CidrBlock" : "0.0.0.0/0",
        "PortRange" : { "From" : "0", "To" : "65535" }
      }
    },
    "PublicSubnetNetworkAclAssociation" : {
      "Type" : "AWS::EC2::SubnetNetworkAclAssociation",
      "Properties" : {
        "SubnetId" : { "Ref" : "PublicSubnet" },
        "NetworkAclId" : { "Ref" : "PublicNetworkAcl" }
      }
    },
    "EC2SecurityGroup" : {
      "Type" : "AWS::EC2::SecurityGroup",
      "Properties" : {
        "GroupDescription" : "Enable access to the EC2 host",
        "VpcId" : { "Ref" : "VPC" },
        "SecurityGroupIngress" : [
          { "IpProtocol" : "tcp", "FromPort" : "22",  "ToPort" : "22",  "CidrIp" : "0.0.0.0/0" },
          { "IpProtocol" : "tcp", "FromPort" : "80",  "ToPort" : "80",  "CidrIp" : "0.0.0.0/0" }
        ]
      }
    },
    "PublicInstance" : {
      "Type" : "AWS::EC2::Instance",
      "DependsOn" : "GatewayToInternet",
      "Properties" : {
        "InstanceType" : "t2.micro",
        "ImageId"  : "ami-9be6f38c",
        "NetworkInterfaces" : [{
          "GroupSet"                 : [{ "Ref" : "EC2SecurityGroup" }],
          "AssociatePublicIpAddress" : "true",
          "DeviceIndex"              : "0",
          "DeleteOnTermination"      : "true",
          "SubnetId"                 : { "Ref" : "PublicSubnet" }
        }]
      }
    }
  }
}
#+END_EXAMPLE

- The overview of the template

[[file://home/crito/Pictures/org/cloudformation_lab0.png]]

*** Lesson 35 - Backup Services on AWS and Services that Include Backups

- RDS backups
  - transaction storage engine is recommended for durability
  - degrades performance if Multi-AZ is not enabled
  - Deleting an instance deletes all automated backups (not manual backups)
  - Backups are stored internally on Amazon S3
    
- RDS restoring
  - When restoring, only the default DB parameter and security groups are associated with the instance
  - You can change to a different DB engine as long as it is closely related to the previous engine and there is enough space allocated
    
- ElastiCache
  - Backups available for Redis clusters only
  - Snapshots backup data for the entire cluster at a spcific point in time
  - Backup window should be during the least-utilized time period of the day
  - Snapshots can degrade performance and should be perfomance on read replicas
    
- Redshift
  - Provides free storage equal to the storage capacity of the cluster
  - Snapshots can be automated or manual, and incremental
  - Restoring snapshots creates a new cluster and imports the data
    
- EC2
  - No built-in automated backup option
  - Snapshots of EBS volumes are incremental and can be automated with the API, CLI, or even AWS Lambda
  - Snapshots cause performance degradation 
  - snapshots are stored on S3
    
*** Lesson 36 - Creating and scripting Automation for EC2 SnapshotsC
    
- packages requred for the script
#+BEGIN_SRC sh
yum install python-pip
pip install boto3
#+END_SRC

- aws configure - settings that the script will use
  - this ceates the .aws/credentials that scripts will use
  #+BEGIN_SRC sh
  aws configure
  #+END_SRC
  - It will prompt for Access Key ID, Secret Access Key, Default Region and Default output format
  - If aws-cli isn't install on the instance the file can be created manually
  ~/.aws/credentials
  #+BEGIN_EXAMPLE
  [default]
  aws_access_key_id = AKIAIMOUZUR4MYVEJCNASE
  aws_secret_access_key = 4hw3RIFMemoI4ffWmbscV2MA28zGpSRyx/
  #+END_EXAMPLE
  
- backup_all_vols.py
#+BEGIN_EXAMPLE
#!/usr/bin/python

import boto3

ec2 = boto3.resource('ec2')

for volume in ec2.volumes.all():
	vol_id = volume.id
	description = "backup-%s" %(vol_id)
	ec2.create_ssnapshot(Volume(d=vol_id, Description=description)
#+END_EXAMPLE

- backup_only_running_vols.py 
#+BEGIN_EXAMPLE
#!/usr/bin/python

import boto3

ec2 = boto3.resource('ec2')

print("\n\nAWS snapshot backup started")
instances = ec2.instances.filter(
	Filters=[{'Name': 'instance-state-name', 'Values': ['running]}])
	
for instance in instances:
	instance_name = filter(lambda tag: tag['Key'] == 'Name', instance,tags)[0]['value']
	
	for volume in ec2.voumes.filter(Filters=[{'Name': 'attachment.instance-id', 'Values':[instance.id]}]):
		description = 'scheduled_snapshot-%s.%s %(instance_name, volume.volume_id)
		
	if volume.create_snapshot(VolumeID=volume_id, Description=description):
		print("Snapshot created with description [%s]" % description)
		
print("\n\nAWS snapshot backups completed")
#+END_EXAMPLE

- backup_retention_check.py
backs up all volumes then checks the age of the volumes and deletes any that are passed retention period
#+BEGIN_EXAMPLE
#!/usr/bin/python

import boto3
import datetime
import pytz

ec2 = boto3.resource('ec2')

print("\n\nAWS snapshot backup started %s" % datetime.datetime.now())
instances = ec2.instances.filter(
	Filters=[{'Name': 'instance-state-name', 'Values': ['running]}])
	
for instance in instances:
	instance_name = filter(lambda tag: tag['Key'] == 'Name', instance,tags)[0]['value']
	
	for volume in ec2.voumes.filter(Filters=[{'Name': 'attachment.instance-id', 'Values':[instance.id]}]):
		description = 'scheduled_snapshot-%s.%s-%s' %(instance_name, volume.volume_id, datetime.datetime.now().strftime("%Y%m%d-%H%M%S"))
		
	if volume.create_snapshot(VolumeID=volume_id, Description=description):
		print("Snapshot created with description [%s]" % description)
		
	for snapshot in volume.snapshots.all():
		retention_days = 15
		if snapshot.description.startswith('scheduled_snapshot-') and ( datetime.datetime.now().replace(tzinfo=None) = snapshot.stat_time.replace(tzinfo=None) ) > datetime.timedelta(days=retention_days):
			print("\t\tDeleting snapshot [%s - %s]" % (snapshot.snapshot_id, snapshot.description))
			snapshot.delete()
		
print("\n\nAWS snapshot backups completed")
#+END_EXAMPLE

*** Lesson 37 - Read Replicas with MySQL RDS Across Regions

RDS Read Replicas Across Regions
  - Disaster recovery
    - Multi-AZ deployments are not enough to protect against entire regions going down
    - We can use read replicas in other regions for HA
  - Cross-reion replicas can help with performance if we have a global audience
	- packets have a shorter distance to travel between DB and the end user
  - Replica lag can be expected to go up since data has to go across regions

The process of setting up Read Replicas can take quiet a bit of time, this is something that is a fore-thought in case of disaster, not something that can be done to avert disaster within minutes.

*** Lesson 38 - Quickly Recoving from Disasters

- A disaster - anything that has a negative impact on business continuity or finances
- if an entire region goes down, how can you recover as quickly as possible?
  - We can use read replicas across regions for our DB
  - We can have a backup to our infrastructure in a geographically seperate location
  - We can have the latest data and configuration available on our backup
	
- Costs
  - Backup resources sit idle and therefore add to our costs
  - With AWS we only pay for the resources that we use
  - We can lower our costs by only provisioning the bare minimum
	- E.g. Run fewer instances but configure Auto Scaling to automatically grow if needed
	  
- Services for on-premises infrastructure with AWS
 - EC2 and EBS
 - S3
 - AWS Import/Export Snowball (large data movement, disks set to you to post back)
 - RDS
 - ELB and Auto Scaling
 - Amazon Storage Gateway (backup data to S3 automatically)
   - Virtual Tape Library
 - CloudFormation
   
- Tools for Recovery
  - EC2 AMIs
  - VM Import/Export
  - For VMWare - we can use the AWS Management Portal for vCenter
  - Direct Connect - on-premises ppp link you and amazon, good for a lot of data but not as much as snowball
  - Amazon S3 Transfer Acceleration
	
- potential issues with replicating data	
  - The distance between our replication sites ca nincrease replica lag
  - Bandwidth limitations can also delay data replication
  - It's important to understand which services have async replication and which have sync replication
	
*** Lesson 39 - Storing Log Files and Backups
	
- Storing Log Files and Backups
  - Centralized logging
	- consolidated logs in one central location
	- analyze, store and modify the data in any way that you need

  - Tools
	- [[file://home/crito/org/tech/monitoring/rsyslog.org][Rsyslog]] is a good tool for this
	- Splunk
	- Kiwi
	- Graylog
	- [[file://home/crito/org/tech/monitoring/elk_stack.org][ELK Stack]]
	  
  - Other types of logging
	- S3 access logs
	  - enable logging on a bucket
      - Requests made to that bucket will be logged and stored on S3
	  - No extra charge, except the extra storage cost for the logs
		
    - CloudTrail
	  - Logs API calls made on our account
	  - Useful for debugging, security auditing, and to learn how users interact with our resources
		
    - CloudWatch logs
	  
*** Lesson 40 - S3 IAM and Buck Policies Concepts
	
- Amazon S3 IAM Policies and Bucket Policies
  - IAM Policy
	- Applies to the user level
	- "User" policy
	  
  - Bucket Policy
	- Applies to the resource level
	- "Resource-based" policy
	  
  - S3
	- Can use buket and user policies
	  - resource-based policies
	  - user policies
    - Bucket permissions specify:
      - who is allowed to access resources
	  - what that user can do with those resources
	- AWS gives full permissions to the owner of a resource
    - Resource owners can grant access to others, even cross-account
	  - The bucket owner paying bills can deny access or modify objects regardless of who owns it
		
  - Bucket Policies
	- resource-based policy
	- used a json file attached to the resource
    - can grant other aws accounts or IAM users persision for the bucket and objects inside
	- should be used to manage ross-account permissions for all Aazon S3
	- limited to 20KB in size
	- Example Bucket Policy  
    #+BEGIN_EXAMPLE
    {
	 "Version":2015-10-02",
	 "Statement": [
	  {
	   "Sid": "AddObject",
	   "Effect": "Allow",
	   "Principal": {"AWS": ["arn:aws:iam::862345521403:user/james"]},
	   "Action": ["s3:PutObject"],
	   "Resource": "arn:aws:s3:::examplebucket/*"
	  }
     ]
	}
    #+END_EXAMPLE
	
  - ACLs
	- used for both buckets and objects
	- grant read/write permissions to other AWS accounts
	- you cannot grant conditional permissions
	- you cannot explicitly deny permissions
	- an object ACL is the only way to manage access to objects not owned by the bucket owner
	- use XML format
	  
  - IAM policies (user-based)
	- user policy
	- can create multiple users and give them the same policy or different policies
	- policies are attached and can be detached
	- cannot grant anonymous users
	- Example IAM policies
	  #+BEGIN_EXAMPLE
	  {
	   "Statement": [
	    {
		 "Effect":"Allow",
		 "Action": [
		  "s3:PutObject",
		  "s3:GetObject",
		  "s3:DeleteObject",
		  "s3:ListAllMyBuckets",
		  "s3:ListBucket"
		 ],
		 "Resource2:"arn:aws:s3:::examplesbucket/*"
	    }
	   ]
	  }
	  #+END_EXAMPLE
	  
  - Specifying Resources in a policy
	- arn:aws:s3:::bucket_name
	- arn:aws:s3:::bucket_name/key_name
	  
    - All object in examplebucket
	- arn:aws:s3:::examplebucket/*

	- All buckets
	- arn:aws:s3:::*

	- Variables
	- arn:aws:s3:::examplebucket/developers/${aws:username}/
	  
*** Lesson 40 - Bucket Policies
	
- Elements of an access policy
  - Resources
    - used to idenfity resources with amazon resource names (ARN)
	  
  - Actions
	- actions we want to allow or deny
	- explicit deny always overfides an explicit allow
	  
  - Effect
	- defines whether to allow or deny the above action
	
  - Principal
	- an account or user that this policy applies
	- specific to S3 bucket policies, not user policies
	  
*** Lesson 41 - Building IAM Policies
	




- IAM Policy Simulator
  - this allows you to check your policies against set actions
	
*** Lesson 42 - Network Access Control Lists (NACLs) and Security Groups
	
- VPC Secuirty
  - Security groups
  - NACLs
	
*** Lesson 42 - Using IAM Roles with EC2
*** Lesson 43 - MFA on Amazon Web Services(Multifactor Authentication)	
	
- AWS MFA to access the console
  - users type in their user and passsword as well as a time-based code
  - the time-based code can be on the user's computer, smartphone, or a device that they carry around
  - this should be turned on for the users who have access to the console
	
- Enable MFA for API access
  - you can protect your resources from unauthorised API calls using MFA
  - with IAM and bucket policies, we can decide which actions require this and for which resources
	
- Integrating MFA with Amazon STS
  - we need to integrate with the security Token Service to receive temporary credentials
	- to do that, our call should include the device identifier for the device associated with our account
	- we also need to include the time-based code generated by our device
	- we then get back our temporary security cedentials that can be used to make requests against AWS Services
  - Policies can check for the presence of the MFA policy or they can force periofic re-authentication
  - Not all services support this - services like Amazon S3, SQS and SNS do support it
	
*** Lesson 43 - Security Token Service
	
- AWS Security Token Service
  - allos you to grant trusted user temporary and controlled access to AWS resources
	
  - Grant temporary access
	- to existing IAM users
	- to web-based identity providers: Facebook|Amazon|Google
	- to your organization's existing identity system
	  
  - Credentials are associated with an IAM access control policy that limits what the user can do
	
  - Amazon STS API
	- AWS SDKs
	- AWS CLI
	- AWS Tools for Windows Powershell
	  
  - STS
	- STS returns temporary security credentials
	  - these consist of an access key and a session token
	- Access key
	  - consists of an access key ID and a secret key 
    - Session Token
	  - used to validate our user's temporary security credentials
	- Credentials expire after a certain amount of time

  - Terms
	- Federation
	  - creating a truct relationship between an identity provider and AWS
	  - Users can sign into an identity provider like Amazon, FB, Google, or any other recognized provider
	- Identity broker
	  - The broker is in charge of mapping the user to the right set of credentials
	- Identity Store
	  - An identity store is something like FB, Google, Amazon or AD
	- Identities
	  - A user or "identity" within an identity store
		
  - Temporary Credentials with Amazon EC2
	- Assign an IAM role to the EC2 instance
	- Get automatic temporary security credentials from the instance metadata using the AWS SDKs/CLI
	- You don't have to explicitly get credentials
	  
*** Lesson 44 - Shared Responsibility Model
	
- Shared Responsibility Env (your end)
  - IAM
  - MFA
  - Password/Key Rotation
  - Access Advisor
  - Trusted Advisor
  - Security Groups
  - Access Control Lists
  - VPC
	
- Shared Responsibility Env (AWS)
  - pyhsical server level and below
  - physical environment security and protection - /fire/power/climate/management
  - storage device decommissioning according to industry standards
  - Network device security and ACL's
  - API access endpoints use ssl for secure communication
  - ddos protection
  - EC2 instances cannot send spoofed data

- port scanning against rules even if it's your own environment 	
- personel access to facilities

- EC2 instance hypervisor isolation  
  - even if instances are on the same physical device, thy are separated at the hypervisor level. They are independent of each other.

*** Lesson 45 - AWS and IT Audits
	
- AWS performs self audits of changes to key services to monitor quality, maintain high stantards, and facilitate continuous improvement of the change management process
  
- For audits, AWS provides:
  - Information regarding their global infrastructure
  - from the host OS and virt layer down to the physical security of facilities
  - AWS provides annual cert and reports: (SOC (Service Organization Control) reports, ISO 27001 cert, PCI assessments)
	
- For audits, the customer provides:
  - anything their organization puts on their AWS assets
  - e.g. OS, apps on VM instances, objects in S3, DB like RDS etc
	
*** Lesson 46 - Route53 and DNS Failover	
*** Lesson 47 - Weighted Routing Policies in Route53	
	
- This ability allows you to determine where traffic is sent based on the DNS settings
- This sits in front of the ELB
  
- good for slow migration to new version of application (70/30, 80/20, 90/10, 100/0)
  
*** Lesson 48 - Latency Based Routing
	
- This is used for multiple region infrastructure
- This uses regions to know what latency to set for the user

*** Lesson 49 - VPC Essentials
	
- VPC resembles
  - private data centers
  - private corporate networks
	
- private network
  - private and public subnets
  - scalable infrastructure
  - ability to extend corporate/home network to the cloud as if it were part of your network
	
- Benefits of a VPC
  - Ability to launch instances into a subnet
  - Ability to define custom ip addr ranges inseide of each subnet
  - Ability to configure route tables between subnets
  - Ability to create a layered network of resources
  - Extending our network with VPN/VPG controlled access 
  - Ability to use Security Groups and Subnet network ACLs
	
- Default VPC
  - default VPC is a different setup than a non-default VPC
  - Default VPC gives users easy access to a VPC without having to configure it from scratch
  - Default VPC subnets have internet gateways attached
  - Each instance added has a default private and public IP address
  - If you delete the default VPC, the only way to get it back is to contact AWS
	
- non-default
  - non-default vpv have private ip addr but not pub ip addr
  - can only access resources through elastic ip addr, VPNs or gateway instances
  - do not have internet gateways attached by default
	
- VPC peering
  - vpc peering allows you to setup direct network routing between different vpc using private ip addr
  - instances will communicate with each other as if thery were on the same private network
  - vpc peering can occur between other AWS accounts and other VPCs within the same region
	
- VPC Limits
  - 5 VPCs per person
  - 200 subnets per VPC
  - 50 Customer gateways per regiion
  - 5 internet gateways per region
  - 5 elastic ip addr per region for each AWS account
  - 50 VPN connections per region
  - 200 route tables per region
  - 500 security groups per region
	
*** Lesson 50 - Building a Non-Default VPC
	
- Don't delete the default VPC, you will have to contact AWS to get a new one
- makesure to use ssh-add, to dperform ssh forwarding to private instances through public instances  
  
*** Lesson 51 - VPC Networking
*** Lesson 52 - VPC Security
	
  Internet Gateway

        Router
		
     Route Table
  
     Network ACL
  
        Subnet
  
   Security Group
   
- Above is the flow of traffic and how security is implemented
- a subnet has to have a acl attached and will use the default if it is the only one available
  
*** Lesson 53 - Configuring a NAT Instance
	
- this instance routes traffic from the private instances to the internet
  - this will allow outside connection to private instances
  - the update of private instances from the external sources(internet/git)

- a special security group needs to be created for the NAT instance
  - the ip table rules need to be set: 
    - to allow the private instances to connect to any external port
	- to allow the private instances ip/subnet to be able to connect
	  
- the source/destination check needs to be disabled on the NAT instance
  
*** Lesson 54 - DB Subnet Groups
*** Lesson 55 - Elastic IP Addresses and Elastic Network Interfaces	
*** Lesson 56 - Configuring Web Appliction in a Non-Default VPC	
	
- first buld the non-default VPC
  - create subnets
	- public and private
  - don't forget about Multi-AZ for failover
  - attach the internet gateway	
	- add route to public subnet
	  
- launch RDS
  - set DB Subnet Group

- launch EC2 instance
  - use git to clone app into new instance
  - use the dep tool to install deps (composer, pip etc)	
  - connect instance to RDS
	- methods for connection will vary with different lang, platform (laravel use .env)
  - Use this instance to create an AMI
	- this AMI we will deploy into the auto-scaling group
	  
- set up security groups for the EC2 to connect to RDS, otherwise the connection will fail
  
- confirm nginx is running
  - move application to correct directory and configure nginx to server application
    - choose between Fastcgi or php-fpm
    - ensure that permissions are correct on the application	
	  
- Create an internate facing LB
  - you will need to ensure that each AZ has a public subnet
  - configure health check setting
	
- Configure Auto-Scaling

*** Lab - Creating a NAT Instance and Gateway in a VPC
Lab Guide: [[file://home/crito/Documents/SysAdmin/Cloud/AWS/LA_Lab_Guide_NAT_in_VPC.pdf][Creating a NAT Instance in a VPC]]

*** Lab - Building a Virtual Private Cloud from Scratch
Lab Guide: [[file://home/crito/Documents/SysAdmin/Cloud/AWS/LA_Lab_Guide_VPC_from_Scratch.pdf][Building a Virtual Private Cloud from Scratch]]

*** Lab - Createing a VPC with CloudFormation and Launching an EC2 Instance
Lab Guide: file://home/crito/Documents/SysAdmin/Cloud/AWS/LA_Lab_Guide_CloudFormation_Walk_Through.pdf

#+BEGIN_SRC json
{
  "AWSTemplateFormatVersion" : "2010-09-09",
  "Description" : "Building A VPC From Scratch With CloudFormation",

  "Resources" : {
    "VPC" : {
      "Type" : "AWS::EC2::VPC",
      "Properties" : {
        "EnableDnsSupport" : "true",
        "EnableDnsHostnames" : "true",
        "CidrBlock" : "10.0.0.0/16",
        "Tags" : [
          { "Key" : "Application", "Value" : { "Ref" : "AWS::StackName" } },
          { "Key" : "Network", "Value" : "Public" }
        ]
      }
    },
  
    "PublicSubnet" : {
      "Type" : "AWS::EC2::Subnet",
      "Properties" : {
      "VpcId" : { "Ref" : "VPC" },
      "CidrBlock" : "10.0.0.0/24",
        "Tags" : [
        { "Key" : "Application", "Value" : { "Ref" : "AWS::StackName" } },
        { "Key" : "Network", "Value" : "Public" }
        ]
      }
    },
    
    "InternetGateway" : {
      "Type" : "AWS::EC2::InternetGateway"
      },
  
    "GatewayToInternet" : {
      "Type" : "AWS::EC2::VPCGatewayAttachment",
      "Properties" : {
        "VpcId" : { "Ref" : "VPC" },
        "InternetGatewayId" : { "Ref" : "InternetGateway" }
      }
    },
    
    "PublicRouteTable" : {
      "Type" : "AWS::EC2::RouteTable",
      "Properties" : {
        "VpcId" : { "Ref" : "VPC" }
      }
    },
  
    "PublicRoute" : {
      "Type" : "AWS::EC2::Route",
      "DependsOn" : "GatewayToInternet",
      "Properties" : {
        "RouteTableId" : { "Ref" : "PublicRouteTable" },
        "DestinationCidrBlock" : "0.0.0.0/0",
        "GatewayId" : { "Ref" : "InternetGateway" }
      }
    },
  
    "PublicSubnetRouteTableAssociation" : {
      "Type" : "AWS::EC2::SubnetRouteTableAssociation",
      "Properties" : {
        "SubnetId" : { "Ref" : "PublicSubnet" },
        "RouteTableId" : { "Ref" : "PublicRouteTable" }
      }
    },
    
    "PublicInstance" : {
      "Type" : "AWS::EC2::Instance",
      "DependsOn" : "GatewayToInternet",
      "Properties" : {
        "InstanceType" : "t1.micro",
        "ImageId" : "ami-fb8e9292",
        "NetworkInterfaces" : [{
          "AssociatePublicIpAddress" : "true",
          "DeviceIndex" : "0",
          "DeleteOnTermination" : "true",
          "SubnetId" : { "Ref" : "PublicSubnet" }
        }]
      }
    }
  }
}
#+END_SRC


*** Lesson 57 - AWS Direct Connect and On-premises to VPC Redundancy
	
- you can connect on-site infrastructure to AWS
  - move business apps to the cloud
  - run analytics

- it is achieved by using VPN
  - adding a Virtual Private Gateway to the VPC that you can connect customer network.
	
- Considerstions
  - you can have 5 VPG per region
  - you can only have 1 VPG per VPC
  - you can have 50 Customer Gateways per region
  - these numbers can be increased by AWS
	
- Bandwidth Considerations
  - most vpn connections cannot support consistent 4Gbps data transfer rates
  - AWS direct connect offers dedicated network connections
	- more badnwidth throughput
	- consistent performance
	- private connection instead of going over the public internet
	- direct connect provides 1Gbps and 10Gbps ports and we can provision multiple connections if we need more capacity

- AWS Direct Connect uses BGP drouting
  - we need to use BGP with ASN and IP prefixes
	
- Creating redundat tunnels
  - if something happens to our first tunnel, we can automatically failover to the second
    - one tunnel is always used and the other is for failover only
    - the customer Gateway must be configured for both tunnels
	  
** Using the EC2 Container Service - Linux Academy
[[file://home/crito/Documents/SysAdmin/Cloud/AWS/LA_EC2_Container_Service/linuxacademy-aws-containers.pdf][EC2 Container Service - Introduction]]
[[file://home/crito/Documents/SysAdmin/Cloud/AWS/LA_EC2_Container_Service/linuxacademy-aws-containers-ecs-limits.pdf][EC2 Container Service - Service Limits]]
[[file://home/crito/Documents/SysAdmin/Cloud/AWS/LA_EC2_Container_Service/linuxacademy-aws-whatiscontainer.pdf][EC2 Container Service - What is a Container?]]

*** Introduction
- What is a Container?
A container is exactly what you might expect it to be based on the general definition of the word. It is an entirely isolated set of packages, libraries and or applications that are completely independent from its surroundings.  

- Container Architecture
  - Docker
    - client-server application where both the daemon and client can be run on the same system or you can connect a Docker client with a remote Docker daemon
      
  - Main Components
    - Daemon
    - Client
    - Docker.io Registry
      
  - Containers rest on top of a single linux instance. This allows the container to leave behind a lot of the bloat associated with a full hardware hypervisor.
    
  - other concepts that are similar to linux containers
    - FreeBSD - Jails
    - Sun Solaris - Zones
    - Google - Imctfy (Let Me Contain That For You)
    - OpenVZ
  
- EC2 Container Service
  - Amazon ECS is highly scalable and fast container management service.
  - Has published API to start and stop container aware applications.
  - Can query applications and instances to get their state, all from a centralized service.

  - ECS components 
    - Clusters
      - This is just a grouping of container instances that we 'do stuff' on
    - Container Instances
      - EC2 instances running the ECS agent and registered in a cluster.
    - Task Dfinitions
      - Description of an appliction with one or more container definitions.
    - Scheduling
      - How we get our tasks on the container instances.
    - Services
      - allows us to run or maintain a number of instances of a task definition
    - Tasks
      - An instance of a Task Definition
    - Containers
      - A Linux Container created as part of the task

*** Setup and Configuration
- Create an ECS User and Group
  - always ensure that you customize the IAM user sign-in link, otherwise by default the account number is used which shouldn't be given to people who are administering AWS
  - Create a new user, makesure that you download a copy of the creds, as the won't be available later
    - this file will be a csv file
  - Create a group specific for ECS
    - there are very specific policies available for ECS (be careful granting full administration rights anyone in the group will automatically have full privs)
    
- Logging into the console
  - to grant access to the AWS console
  - Security Credentials -> manage passwords -> Assign Custom password -> check Require user to create a new password
    - this will allow us to set an initial password, that will force the user to create there own on initial login
      
- Creating Instance KeyPairs
  - EC2 -> Key Pairs -> Create Key Pair -> add name (name-region) -> download key
    - add the ssh key to the key exchange on your machine
      #+BEGIN_SRC sh
      ssh-agent bash
      ssh-add newkey.pem
      #+END_SRC
      
- Creating Cluster VPC
  - this VPC can be created with ECS wizard, but for more granular control you may want to create a none default VPC
    - VPC -> Create VPC -> add name -> add private ip range -> select Tenancy (dedicated adds an ip that can not be assigned to another machine, but this costs)   
      
- Security Groups and ECS Clusters
  - EC2 -> Secuirty Group -> add name -> add description -> choose VPC -> add rules to apply
    
- Install and Configure the AWS CLI (Centos7)
  - first install awscli
    #+BEGIN_SRC sh
    yum install epel-release
    yum install python-pip
    pip install awscli
    #+END_SRC
  - import the .csv file from earlier when creating the user
    #+BEGIN_SRC sh
    aws configure
    #+END_SRC
    - this will prompt for key, scret and region
  - will now be able to connect to aws
    - confirm with a simple command
    #+BEGIN_SRC sh
    aws ec2 describe-regions
    #+END_SRC
    
- Installing Docker for ECS
  - first install docker
    #+BEGIN_SRC sh
    yum install docker
    systemctl enable docker
    systemctl start docker
    #+END_SRC
  - create a docker group
    #+BEGIN_SRC sh
    groupadd docker
    #+END_SRC
  - add user to the docker group
    - this means that root access isn't required to administer docker
  
*** Components and Usage
- The EC2 Container Service Wizard
  - define task
  - configure service
  - configure cluster
  - launch
    
- Using AWS cmd line to communicate with the EC2 Cluster
  - you will require the key-pair that you specified for that region
  - no matter how many instances that you have running the number of services will remain at what was specified
    - 2 instances, but only 1 service (only one of the containers will be accessable, unless ELB is configured)
      
- task definitions can't be deleted once created, this means that a meaningful naming convention is selected.
  
- Service Limits
  - Number of clusters per region, per account   - 1000
  - Number of container instances per cluster    - 1000
  - Number of load balancers per service         - 1
  - Number of tasks per service                  - 1000
  - Number of tasks launched at once             - 10
  - Number of container instances per start-task - 10
  - Throttle on number of container instances    - 5 per cluster
    per second for run-task
  - Throttle on container instance registration  - 1 per second/60 max per minute
    rate
  - Task definition size limit                   - 32 KiB
  - Task definition max containers               - 10
  - Throttle on task definition registration rate- 1 per second/60 max per minute

** AWS Certified Solutions Architect - Associate - Linux Academy
OrionPaper: file:///home/crito/Documents/SysAdmin/Cloud/AWS/cert_solutions_arch/orionpapers_cert_solutions_arch.html
BluePrint: [[file://home/crito/Documents/SysAdmin/Cloud/AWS/aws-certified-solutions-architect-associate-blueprint.pdf][AWS Certified Solutions Architect Associate - Blueprint]]
WhitePapers:
[[file://home/crito/Documents/SysAdmin/Cloud/AWS/cert_solutions_arch/aws-cloud-best-practices.pdf][AWS Cloud Best Practices]]
[[file://home/crito/Documents/SysAdmin/Cloud/AWS/cert_solutions_arch/aws-storage-services-whitepaper.pdf][AWS Storage Services]]
[[file://home/crito/Documents/SysAdmin/Cloud/AWS/cert_solutions_arch/aws-security-best-practices.pdf][AWS Security Best Practices]]
[[file://home/crito/Documents/SysAdmin/Cloud/AWS/cert_solutions_arch/aws-building-fault-tolerant-applications.pdf][AWS Building Fault Tolerant Applications]]
[[file://home/crito/Documents/SysAdmin/Cloud/AWS/cert_solutions_arch/aws-disaster-recovery.pdf][AWS Disaster Recovery]]
[[file://home/crito/Documents/SysAdmin/Cloud/AWS/cert_solutions_arch/aws-emr-best-practices.pdf][AWS EMR Best Practices]]

Subnetting: [[file://home/crito/Documents/SysAdmin/Cloud/AWS/cert_solutions_arch/subnetting-guide.pdf][Subnetting Guide]]

VPC Labs:
file://home/crito/Documents/SysAdmin/Cloud/AWS/cert_solutions_arch/LA_Lab_Guide-VPC_from_Scratch_2.pdf

EC2 Labs:
file://home/crito/Documents/SysAdmin/Cloud/AWS/cert_solutions_arch/LA_Lab_Guide-Provisioning_an_EC2_Instance.pdf
file://home/crito/Documents/SysAdmin/Cloud/AWS/cert_solutions_arch/LA_Lab_Guide-EC2_Backup_Solutions_with_AMIs_and_Snapshots.pdf
file://home/crito/Documents/SysAdmin/Cloud/AWS/cert_solutions_arch/LA_Lab_Guide-Access_Usermetadata.pdf

ELB Labs:
file://home/crito/Documents/SysAdmin/Cloud/AWS/cert_solutions_arch/LA_Lab_Guide-Setting_Up_an_ELB_and_Auto_Scaling_Group.pdf

Bastion Host NAT Gateway Lab:
file://home/crito/Documents/SysAdmin/Cloud/AWS/cert_solutions_arch/LA_Lab_Guide-Bastion_Host_and_NAT.pdf

Connectivity Issue Labs:
file://home/crito/Documents/SysAdmin/Cloud/AWS/cert_solutions_arch/LA_Lab_Guide-TS_Connectivity_Issue_One.pdf
file://home/crito/Documents/SysAdmin/Cloud/AWS/cert_solutions_arch/LA_Lab_Guide-TS_Connectivity_Issue_Two.pdf
file://home/crito/Documents/SysAdmin/Cloud/AWS/cert_solutions_arch/LA_Lab_Guide-TS_Connectivity_Issue_Three.pdf

S3 Labs:
file://home/crito/Documents/SysAdmin/Cloud/AWS/cert_solutions_arch/LA_Lab_Guide-S3_Static_Web_Hosting.pdf
file://home/crito/Documents/SysAdmin/Cloud/AWS/cert_solutions_arch/LA_Lab_Guide-S3_Backup_and_Archiving_Solutions.pdf

Route53 and CloudFront Labs:
file://home/crito/Documents/SysAdmin/Cloud/AWS/cert_solutions_arch/LA_Lab_Guide-Configure_Route53_DNS_Record_Sets.pdf
file://home/crito/Documents/SysAdmin/Cloud/AWS/cert_solutions_arch/LA_Lab_Guide-Configuring_a_Cloudfront_Distribution.pdf

VPC Labs:
file://home/crito/Documents/SysAdmin/Cloud/AWS/cert_solutions_arch/LA_Lab_Guide-VPC_Peering.pdf

RDB Labs:
file://home/crito/Documents/SysAdmin/Cloud/AWS/cert_solutions_arch/LA_Lab_Guide-Create_and_Configure_an_RDS_DB.pdf

CloudFormation Labs:
file://home/crito/Documents/SysAdmin/Cloud/AWS/cert_solutions_arch/LA_Lab_Guide-S3_Bucket_with_CloudFormation.pdf

Elastic Beanstalk Labs:
file://home/crito/Documents/SysAdmin/Cloud/AWS/cert_solutions_arch/LA_Lab_Guide-Deploying_a_Simple_Web_Application_with_Elastic_Beanstalk.pdf

** AWS Security Essentials - Linux Academy
- AWS Security Essentials Course Supplement - https://www.lucidchart.com/documents/view/dfe09ce1-c62b-4834-b2af-15dcba040219/0
  
Docs:
  - [[file://home/crito/Documents/SysAdmin/Cloud/AWS/NIST_Special_Publication-800-88.pdf][NIST Special Publication - 800-88]]
  - [[file://home/crito/Documents/SysAdmin/Cloud/AWS/Security_Best_Practices.pdf][Security Best Practices]]
  - [[file://home/crito/Documents/SysAdmin/Cloud/AWS/National_Industrial_Security_Program-Operating_Manual.pdf][National Industrial Security Program - Operating Manual]]
   
*** Secure Global Infrastructure and Compliance
- regions
  - largest geographical area in AWS
  - largest organizational unit in AWS
    
- Availability Zone
  - a region is broken down into AZs
  - AZs use an alpha code to denote differnet zones i.e us-east-1b region [us-east-1] az [b] 
    
- Endpoints
  - AWS Console
  - AWS CLI
  - AWS API
  - ClouldFront (CDN)
    
- VPC Endpoints
  - this means that services don't have to leave your AWS VPC
  - so an EC2 instances doesn't leave the VPC to access an S3 bucket
  - 2 types
    - interface
      - EC2 API
      - ELB
      - Kinesis Streams
      - EC2 Systems Manager
      - Service Catalog
    - gateway
      - S3
      - DynamoDB
  - Limitations
    - ipv4 only
    - same region only
    - an interface endpoint cannot be accessed through a VPN or VPC Peering connection
      
- IAM
  - Global scope
  - is very granular in how permissions are given to users
  - allows central management of:
    - users
    - passwords
    - access keys
    - permissions
    - groups
    - roles
      
*** Shared Responsibility and Trusted Advisor

- Shared Reponsibility Model
  -Infrastructure services
    - Amazon is responsible for:
      - global Infrastructure (Region, AZs, Edge Locations)
      - foundation Services (Compute, Storage, Databases, Networking)
    - User/Customer is responsible for:
      - customer data
      - platfroms and applications 
      - OS and Network configuration
      - customer iam
  - Container Services
    - Amazon is responsible for:
      - Platforms and applications 
      - OS and network configurations
      - global infrastructure
      - foundation services
    - User/Customer is responsible for:
      - customer data
      - customer iam
  - Abstracted Services(DynamoDB, S3, Lambda)
    - Amazon is responsible for:
      - Network traffic protection
      - Platforms and applications
      - OS and Network configurations
      - global infrastructure
      - foundation services
    - User/Customer is responsible for:
      - data in transit and client-side
      - customer iam
	
- Trusted Advisor (tool)
  - gives reports on AWS environment including:
    - cost optimization
    - performance
    - security
    - fault tolerance
  - services available to all customers
    - six core checks 
      - security groups - lets you know if there are any security holes
      - iam 
      - MFA on root accounts
      - EBS snapshots - avaialbe to the public
      - RDS snapshots - avaialbe to the public
      - service limits - will alert if limits are hit
  - services available to business customers
    - all checks
      - weekly updates
      - get api access to create reporting tools

*** Identity and Access Management (IAM)
- Root User
  - this user is created when an account is created 
  - by default the root user has full admin permissions and access to all the services that account is using
  - best practices:
    - the root user shouldn't be used for daily administration
    - the root user should not have access keys
    - the root user should have to have MFA
      
- Users and Groups
  - a new user has an implicit deny on all services
  - users can have IAM polices directly attached to them or they can be part of a group have as certain policies attached
  - explicit deny always over-rules explicit allow
  - groups allow us to group users together by a criteria and makes administration much easier as user permisions don't have to be set individually
    
- Roles
  - Temporary security creds that are mangaed by STS (Secure Token Service)
  - an entity can assume the permission of the created role for a set amount of time
  - such entities can be:
    - aws resources(such as EC2)
    - a user with our AWS Account that requires permissions for a limited time to a none privileged resource
    - a user outside of the AWS account that requires temporary access(contractor)
    - SAML - this is most commonly seen in Enterprise environments, Directory systems (Active Directory)
  - The STS token can be set from 15mins - 1hr (defaults to 1hr)
  - STS uses a single endpoint
    - https://sts.amazonaws.com - this is in us-east-1
      
- Access Advisor
  - this tool allows us to audit what services a user has accessed
  - with the access information we can make a more informed dicision on what privileges a user requires (principle of least privilege) 
    
*** Encryption Essentials
    
- HSM (Hardware Security Module)
  - physical device used for secure key storage and management
  - dedicated device managed by AWS(they don't have access to keys)
  - In the VPC but seperate from other networks for latency and security
  - the keys are manged and controlled by the user

- KMS (Key Management Service)
  - Managed service that allows you create and control your encrypted keys
  - advantages over HSM:
    - can use IAM policies for KMS access
    - AWS Services intergrate directly with with KMS

**** KMS Intergration with S3
- KMS Integration with S3
Before getting started, make sure you're logged in to the AWS web console and have selected the N. Virginia region (us-east-1). You'll also need a few document files – you can use text files or PDFs that you already have on your computer.

- Create an Encrypted S3 Bucket
From the main AWS page, navigate to S3 from the services menu.
Next, click Create Bucket. For the bucket name, enter mytestbucket followed by several random digits to make it unique. Click Next, then click Versioning and select Enable versioning. Click Save to apply this setting.
Click Default encryption and select AWS-KMS. From the dropdown menu, select aws/s3. This will be our master key name. Click Next.
Finally, make sure both the "Read" and "Write" checkboxes are set on the permissions screen for the bucket. Click Next, review the settings, and click Create bucket.

- Upload a File
From the services menu, navigate to IAM. There will be several errors displayed on the screen, but we can ignore them for this lab. From the menu on the left side of the page, select Encryption keys. Notice the encryption key specified in the previous step hasn't been created. To explore this, we'll navigate back to S3 from the services menu.
Click Upload and select a document from your computer. Click Next, making sure that the owner has both "Read" and "Write" permissions and that public access is turned off, then click Next again. On the properties screen, leave the default settings (encryption will be set to "None") and click Next, then Upload.
From the S3 bucket, select the document you just uploaded. The object's "Overview" screen will indicate that it's encrypted with a KMS key. This happens because we set KMS encryption on the bucket itself.
Navigate back to IAM from the services menu, and select Encryption keys from the menu on the left. This time, we'll see our aws/s3 key, which was created when we uploaded our first file.

- Creating Multiple Master KMS Keys
We can also use the KMS console to create multiple master keys. To start, click Create key at the top of the page. Enter a name of your choosing, such as my_s3_key for the Alias as well as the Description. Click Next Step.
We won't be adding tags, so click Next Step. We'll also skip adding key administrators and usage permissions; click Next Step two more times.
On the final screen, we can review our key policy and click Finish. On the list of keys, we'll now see the my_s3_key we just created.

- Using a Created KMS Master Key
Next, we'll learn how to use the key we created.
Navigate back to S3 from the services menu. Click Create Folder and enter a name like myFolder. Below the name field, choose the AWS-KMS encryption setting and select my_s3_key from the dropdown menu. Click Save.
We'll navigate into our newly created folder by clicking its name in our S3 bucket. Click Upload and select another file from your computer. Click Next, make sure that the owner has "Read" and "Write" permissions and that public access is turned off, and click Next again. Leave the default properties (encryption will again be set to "None") and click Next, then Upload.
Click the object's name once it has been uploaded, and we'll see that it has been encrypted with KMS as well. To check its encryption settings, note the last few digits of the KMS key ID, and compare it to the ID of the original document we uploaded. The keys' IDs will match, meaning that both documents are encrypted using the same key - the default aws/s3 key.
This happens because the bucket policy overrides that of the folder within it. In order to use our created key, we'll need to set encryption on the document itself.
Select the second document you uploaded to S3 (within myFolder) and select the Properties tab at the top of the screen. Click Encryption. Below the AWS-KMS option, select my_s3_key from the dropdown menu. Click Save.
Click the Overview tab at the top of the page. This time, the KMS key ID will be different, indicating that the document has been encrypted with the key we created.

*** OS-Level Access
    - Bastion Host
      - these function as a "jumperbox"
      - allows us to securely access instances in private subnets without making these instances public in anyway
      - Best Practives     
        - deploy in two available zones
        - use auto scaling to ensure the number of bastion hosts
        - access is locked down and only allowed from known CIDRanges
        - ports are limited to only ports the bastion host needs
        - Do not copy keys or access information to the bastion host or any other instance
          
    [[file://home/crito/Pictures/org/bastion01.png]]
   
   - when using bastion hosts we need to set up ssh forwarding 
     #+BEGIN_SRC sh
     chmod 400 key_bastion.pem
     ssh-agent bash
     ssh -A ec2-user@34.222.10.52 # connected to bastion host
     ssh ec2-user@10.11.54.125    # connected to the server in the private subnet 
     #+END_SRC
     - A - use ssh-agent
     - ssh forwarding only allows two hops (bastion -> next server)

*** Data Security
    - Protectecting data at rest
      - S3
        - Permissions
          - Bucket level
          - Rule of least priv
          - MFA delete
        - Versioning
          - store new versions for every modified or deleted file
          - helps with accidental deletion
        - Replication
          - Replication happens across availability zones automatically
        - Backup
          - replication and versioning make backups unneccessary
        - Server-side Encryption
          - use either S3 master-key or KMS master-key
          - assists with accidental data exposure as long as the keys are not compromised
        - VPC Endpoint
          - can use data inside the VPC without making it public
    - Glacier
      - All data stored is encrypted using AES-256
      - Each archive gets a unique key
      - A master key is then created and stored securely
    - EBS
      - Replication
        - EBS stores two copies of each volme in the same Availability Zone
        - Helps with hardware failure, but is not intended to help availability
      - Backup
        - snapshots
        - can use IAM to control access to these snapshot objects
      - Server-side encryption
        - AWS KMS master-key
        - Microsoft Encrypted File System
        - Microsoft Bitlocker
        - Linux dmcrypt
    - RDS
      - Permissions
        - use IAM policies on users, groups and roles to limit access
        - rule of least priv
      - Encryption
        - KMS is integrated for most instance sizes (not t2 micro)
        - MySQL, Oracle and MSSQL have cryptographic functions at the platform level
    - DynamoDB
      - Permissions
        - use IAM policies on users, groups and roles to limit access
        - rule of least priv
      - Encryption
        - Same as RDS, can encrypt at the application layer
        - best practice is to use raw binary or Base64-encoded fields when storing encrypted fields
      - VPC Endpoint
        - can use data inside the VPC without making it public
    - EMR
      - amazon-managed service
        - AWS provides the AMIs
        - EMR instances do not encrypt at rest
      - Data Store
        - S3 or DynamoDB
        - HDFS(Hadoop Distributed File System), if used AWS defaults to Hadoop KMS
      - Techniques to improve data security
        - S3 servre-side encryption
        - Application-level encryption
        - hybrid

*** OS Security
    - Custom AMIs
      - AMIs can be public or private
      - Allows for a base configuration to be deployed on instances
        - OS
        - Apps
        - Security settings(authorized_keys, loocalaccounts, file and directory perms)
      - Before publishing an AMI:
        - disable any insecure applications (telnet)
        - disable all ports that are not necessary
        - protect credentials:
          - access keys, certs, or third-party creds are deleted
          - software shouldn't be using default accounts
          - ssh keys must not be published
          - disable guest account(windows)
        - Protect data:
          - delete shell history and logs(event log on windows)
        - remove printer and file sharing or any other sharing service that is on by default(windows)
    - AWS System Manager Features
      - Resource Groups
        - Allows you to group your resources logically(Prod, Test, Dev, DMZ)
      - Insights
        - Aggregates CloudTrail, CloudWatch, TrustedAdvisor, and more into a single dashboard for each resource group
      - Inventory
        - A listing of your instances and software installed on them.
        - Can collect data on applications, files network configs, services and more
      - Automation
        - Automate IT operations and management tasks through scheduling triggering from an alarm or directly
      - Run Cmds
        - secure remote managerment replacing need for bastion hosts or ssh
      - Patch Manager
        - helps deploy OS and software patches across EC2 or on-prem
      - Maintenance Window
        - Allows for scheduling administrative and maintenance tasks
      - State Manager and Parameter Store
        - Used for configuration management
**** 
      - Preparing an Instance for a Custom AMI
      In this lab, we learn how to prepare an EC2 instance we want to create an AMI out of so that no sensitive information is being shared.
      
      Before getting started, make sure you're logged in to the AWS web console and have selected the N. Virginia region (us-east-1).
      
      - Configure the EC2 Instance
      To begin, navigate to EC2 from the services menu, go to the Instances section, and select AMISource from the list of instances. Copy its IPv4 public IP address to your clipboard.
      
      Next, open a terminal window and connect to the instance via SSH:
      
      #+BEGIN_SRC sh
      ssh cloud_user@12.34.56.78
      #+END_SRC
      Be sure to substitute the actual IP address of the instance for the one in the command. The instance login password can be found on the lab page with the other credentials for this lab.
      
      - Configure the AWS CLI
      Once logged in, run the following command:
      
      #+BEGIN_SRC sh
      aws configure
      #+END_SRC
      We'll be prompted for an Access Key ID and a Secret Access Key, both of which can be found on the lab page with the other credentials for this lab. For the default region name, enter us-east-1. Press enter when prompted for the default output format.
      
      - Remove Sensitive Data
      Before we create our AMI, we need to ensure it will not contain any sensitive information. Start by checking the contents of the cloud_user home directory:
      
      #+BEGIN_SRC sh
      ls -la
      #+END_SRC
      SSH Keys
      First, let's examine our .ssh directory. Navigate there and check the contents:
      
      #+BEGIN_SRC sh
      cd .ssh
      ls
      #+END_SRC
      We'll see that this directory includes a private key file, myPrivateSSHKey.pem, which should not be included in our AMI. To securely delete this file, run:
      
      #+BEGIN_SRC sh
      sudo shred myPrivateSSHKey.pem
      rm myPrivateSSHKey.pem
      #+END_SRC
      Next, let's check the contents of the other file in the .ssh directory, authorized_hosts:
      
      #+BEGIN_SRC sh
      more authorized_hosts
      #+END_SRC
      The file contains two public keys. The second one, myUnusedKeyPair is no longer in use, so we should remove it before creating our AMI. Open the file in a text editor:
      
      #+BEGIN_SRC sh
      vim authorized_hosts
      #+END_SRC
      Remove the second keypair, save and exit the file.
      
      - AWS Configuration
      Navigate back up to the cloud_user home directory and into the .aws directory:
      
      #+BEGIN_SRC sh
      cd ..
      cd .aws
      ls
      #+END_SRC
      This directory contains configuration for the AWS CLI that we set up earlier. Check the contents of the credentials file:
      
      #+BEGIN_SRC sh
      more credentials
      #+END_SRC
      We'll see that the credentials are stored on the instance in plain text. The simplest way to remove this information is to run aws configure again. This time, enter "None" at each prompt rather than the actual keys and region information. We can then run more credentials again, and we'll see that the access keys have been removed.
      
      - Bash History
      Finally, we'll check our Bash history for sensitive information. Navigate back up to the cloud_user home directory:
      
      #+BEGIN_SRC sh
      cd ..
      #+END_SRC
      We want to ensure that we're clearing the history for not only the current session, but for all sessions. Run the following commands to clear both of these, respectively:
      
      #+BEGIN_SRC sh
      history -c
      history -w
      echo "" > .bash_history
      #+END_SRC
      If we check our history after running these commands, we'll see only the commands that have been run since clearing the current session's history:
      
      #+BEGIN_EXAMPLE
      $ history
        1 history -w
        2 history
      #+END_EXAMPLE
      - Create an AMI
      Now that we've removed sensitive data from our instance, we can create an AMI from it.
      
      Go back to the EC2 dashboard in the AWS web console and select the AMISource instance. Click Actions at the top of the page, select Image, and then select Create Image. Enter "myAMI" for the image name and click the Create Image button.
      
      From the menu on the left side of the page, select AMIs. We should see the myAMI image we just created in this list. We can now use this image to create launch configurations or launch new instances.
      
      - Review
      In this lab, we learned how to identify and remove some commonly seen pieces of sensitive data from an EC2 instance so that we can create a more secure AMI from it.
      
      Congratulations! You've successfully completed the lab on preparing an instance for a custom AMI!
         
*** Infrastructure Security
    - VPC
      - Internet only
        file://home/crito/Pictures/org/aws_sec_vpc1.png

        - Scenarios
          - No connection between your infrastructure and the VPC
          - No on-premises infrastructure
        - Best Practice
          - use ssl/tls endpoints for applications
          - build your own vpn solutions
          - routing and placement must be planned(public and private subnets)
          - security groups and network access control lists (NACLs)

      - IPSec tunnel over internet
        file://home/crito/Pictures/org/aws_sec_vpc2.png
        
        - Scenarios
          - Your Organisation requires secured communications
          - Lesser need for dedicated throughput
        - Best Practice
          - Deploy vpn using standard AWS vpn components (vpn gateway, customer gateway, vpn connection) 
          - can also use custom vpn solutions if required
          - vpc networking(subnets, security groups, NACLs)

      - AWS Direct Connect
        file://home/crito/Pictures/org/aws_sec_vpc3.png
      
        - Scenarios
          - Your organisation requires dedicated links to peer to AWS
          - There is a need for dedicated throughput(1Gbps and 10Gbps)
        - Best Practice
          - Using a private peered connection like this might not need additional security
          - check your organisation's requirements
          - vpc networking(subnets, security groups, NACLs)

      - Hybrid
        file://home/crito/Pictures/org/aws_sec_vpc4.png
        
        - Scenarios
          - Your organisation might have Direct Connect, but some users connect with internet only
          - Your organisation might require IPSec tunnels be used over Direct Connect
        - Best Practice
          - refer to best practices for each type of connection  you are using in a hybrid environment
          - vpc networking(subnets, security groups, NACLs)
            
    - Network Segmentation
      - VPC
        - isolate workloads into separate VPCs (based on application, department, test, dev, etc)
      - Security Groups
        - group instances with similar functions
        - stateful = every allowed TCP or UDP port will be allowed in both directions
      - NACLs
        - stateless - inbound and outbound rules are separate, no dependencies
        - granular cntrol over IP protocols(allow and deny rules for inbound and outbound evaluated in order)
        - work with security groups (NACL applies for the subnet, security groups apply to members)
        - ephemeral ports - client requests depending on OS(ports1024-65535)
      - Host-based Firewall
        - OS-level firewalls
          
    - Protecting Web Applications
      - AWS Web Application Firewall(WAF)
        - Allows for conditions or rules to be set on web traffic on CloudFront or an Application Load Balancer
        - WAF can watch for x-site scripting, ip addrs, location of requests, query strings and SQL injection
      - AWS Shield Standard
        - The basic level of DDos protection for your web application
        - included with WAF, no additional cost
      - AWS Shield Advanced
        - Expands services protected to include Elastic Load Balancers, CloudFront Distributions, Route53 hosted zones and resources with Elastic IPs
        - Contact 24x7 DDos reponse Team (DRT) for assistance during an attack
        - Some cost protection against spikes in a bill from DDos attacks
        - Expanded protection against many types of attacks
        - WAF is included in Shield Advanced pricing




* Books
[[file://home/crito/Documents/SysAdmin/Cloud/AWS/AWS_Storage_Services.pdf][AWS Storage Services]]
[[file://home/crito/Documents/SysAdmin/Cloud/AWS/LA_Lab_Guide_CloudFormation_Walk_Through.pdf][Linux Academy - Lab Guide CloudFormation Walk Through]]
[[file://home/crito/Documents/SysAdmin/Cloud/AWS/LA_Lab_Guide_NAT_in_VPC.pdf][Linux Academy - Lab Guide NAT in VPC]]
[[file://home/crito/Documents/SysAdmin/Cloud/AWS/LA_Lab_Guide_VPC_from_Scratch.pdf][Linux Academy - Lab Guide VPC from Scratch]]
** SysOps Associate
[[file://home/crito/Documents/SysAdmin/Cloud/AWS/sysops/AWS_Auditing_Security_Checklist.pdf][AWS Auditing Security Checklist]]
[[file://home/crito/Documents/SysAdmin/Cloud/AWS/sysops/AWS_Backup_Recovery.pdf][AWS Backup Recovery]]
[[file://home/crito/Documents/SysAdmin/Cloud/AWS/sysops/AWS_Building_Fault_Tolerant_Applications.pdf][AWS Building Fault Tolerant Applications]]
[[file://home/crito/Documents/SysAdmin/Cloud/AWS/sysops/AWS_certified_sysops_associate_blueprint.pdf][AWS Certified SysOps Associate Blueprint]]
[[file://home/crito/Documents/SysAdmin/Cloud/AWS/sysops/AWS_Cloud_Architectures.pdf][AWS Cloud Architectures]]
[[file://home/crito/Documents/SysAdmin/Cloud/AWS/sysops/AWS_Disaster_Recovery.pdf][AWS Disaster Recovery]]

* Links
