#+TAGS: cloud aws


* Amazon Web Services
HomePage: https://aws.amazon.com/
AWS Documentation: https://aws.amazon.com/documentation/
* Description
* Components
** VPC
** EC2
** S3
** RDB
** Dynamo
** SNS
** Cloudwatch
** ELB
** Route53
** Lambda
* Lecture
** [[https://www.youtube.com/watch?v%3DeKyS9rvbj40][Introduction to Database Services - Brian Rice]]
+ If you host your DB on-site
  - App optimization
  - Scaling
  - HA
  - DB bakups
  - DB patches
  - OS patches
  - OS installation
  - Server maintenance
  - Rack and stack
  - Power, HVAC, networking

+ Host DB in Amazon EC2
You:                     AWS:
  - App optimization       - OS installation
  - Scaling                - Server maintenance
  - HA                     - Rack and stack
  - DB bakups              - Power, HVAC, networking
  - DB patches
  - OS patches
    
+ Host DB with managed DB service
You:                        AWS:
  - App optimization           - Scaling
                               - HA
                               - DB bakups
                               - DB patches
                               - OS patches
                               - OS installation
                               - Server maintenance
                               - Rack and stack
                               - Power, HVAC, networking
				 
+ Managed Services
  - DynamoDB - NoSQL
  - RDS - SQL database engines
  - ElastiCache - In-memory cache
  - Redshift - Data warehouse
- These services provide alternatives to ssh to access the DB    

+ DynamoDB
  - Simple and fast to deploy
  - Simple and fast to scale
    - To millions of IOPS
  - Data is automatically replicated
  - Fast, predicatable performance
- No cost to get started; pay only for what you consume
  
+ Amazon RDS
  - Amazon Aurora, MySQL, PostgreSQL, Oracle, SQL Server
- No cost to get started; pay only for what you consume
- Choose a DB instance type with the right amount of CPU and memory
- Automated bakups
  - restore your db to a point in time
  - enabled by default
  - choose a retention period, up to 35 days
- Manual snapshots
  - persist until you delete them
  - stored in Amazon S3
  - Build a new DB instance from a snapshot when needed
- choose Multi-AZ
  - Availability Zone is a physically distinct independent infrastructure
  - Failover occurs automatically in response to the most important failure scenarios
  - failover usually under 90secs

+ ElastiCache
  - High performance, resizable in-memory caching
  - memcached and Redis engines
  - ElastiCache is wrapper around either of the above engine that is chosen
    
+ Redshift
  - Petabyte-scale columnar DB
  - Fast response time
    - ~ 10x that of typical relational stores
- Pricing $1,000 per TB per year
  - Uses PostgrsSQL JDBC/ODBC
  - is built to reduce I/O
    - data compression
    - zone maps
    - direct-attached storage

** [[https://www.youtube.com/watch?v%3DezpMM1dzN68][Using Domain Verification with Amazon Simple Email Service]]
Back in 2012 Free Tier
  - 2000 messages for free each day when you call SES from your EC2 instance or through AWS Elastic Beanstalk
    
SES - Getting Started
The Amazon SES Sandbox
 - verifying Senders(email accounts that are going to send mail)
 - in the sandbox 
   - 200 messages per 24 hours
 - in production
   - quota - starts at 10000 per day
   - still can only send from verified emails
** [[https://www.youtube.com/watch?v%3DVC0k-noNwOU][Amazon S3 Masterclass - Ian Massingham]]
- Secure
- Durable
- Highly-scalable object storage
- Store and retrieve

+ Use Cases
  - Backup & Archiving
  - Content Stroage & Distribution
  - Big Data Analytics
  - Static Website Hosting
  - Cloud-native Application Data
  - Disaster Recovery
    
*** Fundimental Concepts
  - Buckets
    - Containers for objects stored in S3
    - Consist of data & metadata
    - Combination of a bucket, key & version Id uniquely identify each object.
  - Regions
    - The geographical region where Amazon S3 will create your bucket
    - Will never leave that region, unless you move them
  - Web Store not a file system
  - APIs - http://aws.amazon.com/documentation/s3
  - SDKs - http://aws.amazon.com/tools/
  - Acces via AWS CLI - similar to bash cmds - ls, cp, mv, rm etc

+ Difference between fs and web store
  - write once, read many times (S3 reading more than writing)
  - Eventually consistent

+ Namespaces
  - Object key
    - Max 1024 bytes UTF-8
    - Unique within a bucket
    - Including 'path' prefixes
example - assets/js/jquery/plugins/jtables.js

+ Throughput Optimisation
  - S3 automatically partitions based upon key prefix
    
+ Access Controls
  - IAM Policies
    - fine grained control
    - Administer as part of role based access
    - Apply policies to S3 at role, user and group level
  - Bucket Policies - allow anonymous persons access to a bucket, a class etc
    - Fine grained
    - Apply policies at the bucket level in S3
    - Incorporate user restrictions without using IAM
  - ACLs
    - Coarse grained
    - Apply access control rules at the bucket and or object level in S3
*** Getting Started
- Class of storage
  - Standard - 99% durability and 99% availability
  - Reduced Redundancy Storage - reduced cost, but at lower levels of redundancy
  - Glacier - archiving data, where data access is infrequent and retrieval time of several hours is acceptable.
            - very low-cost
class can be specified on the aws cli 
#+BEGIN_SRC sh
aws s3 cp aws_uki.txt s3://aws-ianm-s3-masterclass/ --storage-class REDUCED_REDUNDANCY
#+END_SRC
class can also be changed in the AWS console(web interface)

- Encryption
  - Securing Data in Transit
    - SSL over HTTPS
    - Alternatively use a client encryption lib such as the Amazon S3 encryption client to encrypt your data before uploading to Amazon S3
      - this is done with a one time encryption key
  - Server Side Encryption (SSE) - 3 options
    - SSE-S3 key management - Amazon mgmt of keys
    - SSE-C - Customer-Provided Keys - Amazon disgards the key
    - AWS KMS (SSE-KMS) - this is a stand alone service
      - S3 with encrypt your data at rest using keys that you manage in the AWS key mgmt service (KMS)
      - KMS provides audit trail to see who used your key to access which object
	
- Audit logs
  - access logs can be created per bucket
    
- Multi-Factor Auth Delete

- Time-Limted Access to Objects
  - time limited urls to allow access to an object for a set time
    
- Versioning & Cross Region Replication
  - Bucket level
    - automatically preserves all copies of objects
  - Persistent
Versioning will increase costs, due to storing multiple copies of objects

- Lifecycle Rules
  - moving S3 buckets to glacier after a certain period of time
    - example would be transaction data after 30 days
  - deleting objects after a certain period of time
    - example would be logs after 30 days

- Website Hosting
  - you can host your entire static website on Amazon S3
* Tutorials
** AWS Foundations - CBT Nuggets
*** How to build a cloud presence
1. Going to the cloud: Traditional Method
Build your own cloud placing your equipment in a data center.
2. Going to the Cloud: AWS Method
Use AWS services to create your infrastructure.

**** Traditional Method
+ Setting up
  - Select a Data Center
  - Purchase Rack Space
  - Purchase Internet Connectivity
  - Install Equipment
    - Switches
    - Firewalls
    - Servers
    - Storage - SAN or NAS
  - Configure Services
  - Expand to More Data Centers - Locality is important when it comes to serequipmentvices such as VOIP
+ Pros & Cons:
  - Massive up-front cost, BIG "Steps"
  - IT Staff: focus on the data center 
  - In-House knowledge limits
  - recreate the wheel
  - It's yours
  - "Monster Server" Capabilities

**** AWS Method
+ Setting up
  - Pick your region
  - Pick your availability zone - these are physical data centers
    - for redundancy you should look at rolling out in to more than one zone
  - Provision your server
  - Configure services
  - Expand to other availability zones
  - Expand to other regions
+ Pros & Cons:
  - Pay As You Go; Pricing Models
  - Elastic Computing; Grow as needed
  - Economy of scale
  - Immediate security accreditation
  - Multiple data centers easily
  - Collaborative innovation
  - horizontal scaling

**** Vertical Vs Horizontal Scaling
***** Vertical Scaling 
  - Increasing HW
  - Increasing Capacity
  - Easy to do

***** Horizontal Scaling (scale out):
  - Increasing instances
  - Shared capacity
  - Typically requires planning

*** Getting Started with AWS
**** What you need to get started
- A Purpose
- Logon Information/Email Address
- A Credit Card/Phone number
***** An Understanding of the services
  - Cloudwatch 
    - Monitors all of the services
    - Can start to get expensive
  - EC2 - Elastic Compute Cloud
    - allows os templates to be created with specific functions db, web etc
    - public available timeplates
  - S3 - Simple Storage Service 
    - written to at least two places
    - Where your image is held whilst not being run
  - EBS - Elastic Block Store
    - faster than S3
    - optional to running image on the ephemeral memeory of the server
      - means that when the machine is shutdown it's data isn't lost
  - Route 53
    - create name records for your domains
    - manages dns
  - VPC - Virtual Private Cloud
    - site to site cloud
  - Auto Sacling
    - amazon automated server management tool
    - will spin up servers when certain limits are reached 
    - shutdown instances when website hits a lull
  - CloudFormation
  - IAM - Identity and Access Management
    - create credentials to access system
  - ELB - Elastic Load Balancing
  - SimpleDB/DynamoDB
    - simpleDB now discontinued
    - Dynamo is really fast
***** AWS Management Console
*** Creating an EC2 Instance - AMI Selection (Amazon Machine Image)
+ Considerations for Provisoning Instances
- In a region all availability zones are connected by high speed fiber.
- Between reigons you are running over the internet and this becomes the dependent factor for data transfer and you should be aware.
- AMI can come with software installed, LAMP, SQL Server etc
- Customized AMIs are stored in S3, this is charged.
- How many instances ?
- Instance type? - micro is available on the Free Tier
- AWS Market Place sells AMIs from different providers offering different software.

*** Understanding EC2 Pricing Models
**** On-demand Instance Pricing
- no commit model
- costs a little more due to this fact
- pricing fluctuates with region
**** Reserved Instance Pricing    
- 1 or 3 year term contract that will lower the rate paid/hour on instances
- Types - these are nothing to do with box performance
    - Light
    - Medium
    - Heavy
The difference in these types is the costing, light you pay less up front but your hourly rate is higher, and Heavy is the opposite, more up front but less per hour.
**** Spot Instance Pricing
- Bidding on left over CPU memory that the data center has available
- But if out bid you lose your resources are shutdown
- The more requirements adds to the chance that you will lose your instance if it is accepted at all.
*** Understanding EC2 Instance Types
**** Measureing Instance Types
+ Instance Types always include a mix of:
  - Memory
  - Processing Power
  - Storage
  - I/O performance
    
+ Instance Families
  - Micro
  - Small
  - Medium
  - Large
  - Extra Large

+ Specific cases
  - High Memory
  - High CPU
  - Cluster

+ Amazon Best Practice: Start small, benchmark and scale up in necessary

**** Understanding Processing Power Ratings
- Everything in AWS is "Virtual" but there really are physical items!
- To provide consistant performance, created the EC2 Compute Unit (ECU)
  - is equivalent to a 1.0 to 1.2 GHz 2007 Xeon Processor
  - it is then split over the number of cores specified by the type
    
**** Understanding I/O Rating
- I/O ratings measure shared resources(Network/Disk)
- Equal shares given to the instances
- I/O Levels
  - Low
  - Moderate
  - High
  - Very High
- Heavy disk performance can benefit from a RAID 0 set across 4 disks
  - obviously risk that comes with RAID 0 one failed disk all gone!!
*** Understanding Tags and Key Pairs
+ Tags
- Tags are a way to identify instances    
  - develop a logical naming convention
- These tags appear on the instance dashboard
- Show/Hide button allows you to customize which tags are visable.
  
+ Key Pairs
- These are the pub/priv key pair that are issued by AWS.
- Windows Key Pair
  - this key gives you the default windows password
  - you get this by right clicking on the window instance and click on "Get Windows Password"
  - you will then be challenged for the priv key to unlock the encrypted password.
- Linux Key Pair
  - this is how you will connect to the machine unless you change the key 
    
+ What if I lose my key?
- Amazon has no way for you to get your priv key again.
- If you have an instance that you need to access, you will need to create an AMI of that instance and recreate it. All of your data will be there but it my require some admin, such as remounting of disks etc.

*** Understanding Security Groups
**** Security Groups: Your EC2 Firewall
- Inbound filtering for your instances
- "Security Groups" - can be individual (Group of one) or multiple
- By default - 
  - Rules: No traffic inbound, all traffic outbound, all traffic within group
- Changing security groups can only be done inside VPC
- Good practice to split DB and Web servers into different secuirty groups
- Don't open RDP(3389) to the world lock it down to your ip, like you do with linux ssh.

*** Understanding Elastic IPs and ELB

*** SES, SNS, SQS
SES - Simple Email Service
  - AWS service allowing you to send email from hosted applications
  - Designed for bulk service
  - Leverages AWS email reputation, volume
  - Outbound scanning on all email sent
  - Uses AWS closed-loop system
  - Accounts limited to 10,000 emails/day, quantity automatically increases
  - Charged based on quantity of email sent
    
SNS - Simple Notification Service
  - Message transmission for humans and services
  - Protocols: HTTP/HTTPS, Email, SMS, SQS
  - SNS Topic created, subscribers added, AWS services report to a topic
  - As with everythin, pay-as-you-go... first million API requests/month free

SQS - Simple Queue Service
  - Message Queuing System
  - Allows you to build applcations without concerns of how communication is stored or handled
  - Unlimited messages, unlimited queue size
  - Message payload up to 25KB 
  - $0.50 / million SQS requests

** AWS Concepts - Linux Academy
[[file://home/crito/Documents/SysAdmin/Cloud/AWS/aws-concepts-pps.pdf][AWS Concepts PPS]]

** AWS Essentials - Linux Academy
http://bit.ly/2guw5giiiY
** AWS Certified SysOps Administrator
[[file://home/crito/Documents/SysAdmin/Cloud/AWS/sysops/AWS_Auditing_Security_Checklist.pdf][AWS Auditing Security Checklist]]
[[file://home/crito/Documents/SysAdmin/Cloud/AWS/sysops/AWS_Backup_Recovery.pdf][AWS Backup Recovery]]
[[file://home/crito/Documents/SysAdmin/Cloud/AWS/sysops/AWS_Building_Fault_Tolerant_Applications.pdf][AWS Building Fault Tolerant Applications]]
[[file://home/crito/Documents/SysAdmin/Cloud/AWS/sysops/AWS_certified_sysops_associate_blueprint.pdf][AWS Certified SysOps Associate Blueprint]]
[[file://home/crito/Documents/SysAdmin/Cloud/AWS/sysops/AWS_Cloud_Architectures.pdf][AWS Cloud Architectures]]
[[file://home/crito/Documents/SysAdmin/Cloud/AWS/sysops/AWS_Disaster_Recovery.pdf][AWS Disaster Recovery]]

*** Lesson 3 - Understanding AWS Instance Types, Utilization and Performance
- Virtualization Types
  - HVM AMIs (Hardware Virtual Machine)
    - Can use special hardware extensions
    - Can use PV drivers for network and storage
    - Usually the same or better performance than PV alone

  - PV AMIs (Paravirtual)
    - Historically faster than HVM, but no longer the case
    
- Instance Types
  - General Prupose
    - T2
      - intended for work loads that do not use the full CPU often or consistently
      - Provided Burstable Performance
    - M3
      - Provide a balance of compute, memory and network resources
      - SSD Storage (Instance store)
    - M4
      - Provide a balance of compute, memory and network resources
      - Support Enhanced Networking
      - EBS-optimized (doesn't allow ssd storage)
	
  - Compute Optimized
    - Lowest price/compute performance in EC2
    - C3
      - SSD-backed instance storage
      - Support for enhanced networking and clustering
    - C4
      - Latest generation of compute-optimized instances
      - hightst performing processors (optimized specifically for EC2)
      - support for enhanced networking and clustering
      - EBS-optimized
	
  - Memory Optimized (big data, such as spark)
    - Lowest price per amount(GiB of RAM) and memory performance
    - R3
      - SSD-backed instance storage
      - High memory capacity
      - Support for enhanced networking
	
  - GPU 
    - Graphics and general purpose GPU compute
    - G2
      - High frequency processors
      - high-performance NVIDA GPUs
      - On-board hardware video encoder
      - Low-latency frame capture and encoding, enabling interactive streaming
      - Useful for GPU compute workloads, machine learning, video encoding 3D application streaming, etc...
	
  - Storage Optimized (Hadoop, data warehousing, MongoDB)
    - Very fast SSD-backed instance storage optimized for high random I/O performance and high IOPS
    - I2
      - high I/O performance
      - high frequency processors
      - ssd storage
      - supports TRIM (free up space)
      - supports enchanced networking
	
- Burstable Performance
  - cpu credits are used to burst past the baseline performance up to 100% of a cpu core
  - credits are gained every hour
  - aws provides an initial amount to ensure that the cpu isn't struggling at start up

*** Lesson 4 - EC2 Instance and System Status Checks

- System Status Checks
  - Loss of network connectivity
  - Loss of system power
  - Software issues on the physical host
  - Hardware issues on the physical host
    
  - Solutions
    - Stop and start instances
    - Terminate and re-launch instances
    - Contact AWS
      
- Instance Status Checks
  - Failed system status checks
  - Incorrect networking or startup config
  - Exhausted Memory
  - Corrupted file system
  - Incompatible kernel
    
  - Solutions
    - Solve what is causing the issue
    - Stop and start instances
    - Terminate and re-launch instances with more memory, a different kernel, or different networking config
      
*** Lesson 5 - CloudWatch Alarms

Alarm state
  - OK           - is within defined thershold
  - ALARM        - is outside of thershold
  - INSUFFICIENT - alarm has just been started, or has insuffiecient data to accurately report
    
- CloudWatch doesn't have metrics for memory, this requires scripts to be provided on the instance
  
- Under Rules you can create cron jobs
  
*** Lesson 6 - Installing and Configuring Monitoring Scripts for EC2 instances

- The scripts will require the permissions to access CloudWatch
  
- CloudWatch will report information at 5min intervals for more detailed reporting you need to enable detailed monitoring.
  - Detailed monitoring is a chargable service (reports every one minute)
    
- install perl, get the monitoring scritps, unzip and run the mon-put-instance-data.pl script
#+BEGIN_SRC sh
sudo yum install perl-Switch perl-DateTime perl-Sys-Syslog perl-LWP-Protocol-https
curl http://aws-cloudwatch.s3.amazonaws.com/downloads/CloudWatchMonitoringScripts-1.2.1.zip -O
unzip CloudWatchMonitoringScripts-1.2.1.zip 
./mon-put-instance-data.pl --mem-util --mem-used --mem-avail --swap-util --swap-used --disk-space-util --disk-space-used --disk-space-avail --memory-units=megabytes --disk-space-units=gigabytes --disk-path=/dev/xvda1
#+END_SRC
- A mon-get-instance-stats.pl is also provided, this script allows us to pull data
  
- also set the mon-put-instance-data.pl to a cron job
#+BEGIN_EXAMPLE
*/5 * * * * ~/aws-scripts/mon-put-instance-data.pl --mem-util --mem-used --mem-avail --swap-util --swap-used --disk-space-util --disk-space-used --disk-space-avail --memory-units=megabytes --disk-space-units=gigabytes --disk-path=/dev/xvda1
#+END_EXAMPLE
these metrics will now be able to be viewed on the dashboard under linux metricsd

*** Lesson 6 - Dedicating an Instance to Monitoring
*** Lesson 7 - Monitoring EBS for Performance and Availability
    
- EBS uses IOPS (I/O operations per second) as a performance measure
- IOPS measured in 256 KiB (Kibibytes) chunks of I/O operations for SSDs
  - SSDs deliver constant preformance for both random and sequential I/O operations
  - 4000 IOPS can transfer 4000 256KiB chunks per second
  - 5 I/O operations at 54KiB will count as 5 operations
- IOPS measured in 1024 KiB chunks of I/O operations for HDDs
  - HDDs have optimal performance with large and sequential I/O operations
  - 8 sequential 128KiB operations will count as 1 operation
  - 8 random 128KiB operations will count as 8 operations
    
- SSD-backed volumes
  - Two different types of SSD volumes: io1 and gp2
  
  - gp2 - General Purpose(default)
    - Baseline performance of 3 IOPS per GB up to 10,000 IOPS
    - Minimum of 100 IOPS (ie: 8 GB volume has 100 IOPS instead of 24)
    - The larger the volume, the more IOPS
    - Can burst up to 3000 IOPS if the size is under 1TB
    - up to 160 MiB/s of throughput
      
  - volumes get credits at the 3 IOPS per GiB of volume size per second
    - volumes start out with their maximum amount of 5.4 million I/O credits
    - running out of credits causes the volume to revert back to baseline IOPS performance
      
  - io1 - Provisioned IOPS
    - ideal for IOPS-intensive and troughput intensive workloads (like db)
    - Baseline prformance of 30 IOPS per GB up to 20,000 IOPS
    - Does not use credits to burst above baseline performance, instead it gives a consistent IOPS rate
    - Delivers within 10 percent of provisioned IOPS performance 99.9. percent of the time in a given year
    - up to 320 MiB/s of throughput
      
- HDD-backed volumes
  - Throughput Optimized HDD (st1 and Cold HDD (sc1)
    - can sometimes provide more throughput (MB/s) but drastically less IOPS

  - Throughput Optimized HDD - st1	
    - ideal for frequently accessed and throughput intensive workloads

  - Cold HDD - sc1 
    - less frequently accessed workloads
    - lowest cost HDD volume
      
- Performance - Pre-warming/initialization
  - initialisation is no longer needed for new EBS volumes
    - EBS volumes get maximum performance right away
    - Storage blocks on volumes restored from snapshots do need to be initialized
      
  - initialisation can be accomplished by reading from all blocks on a volume with dd or fio utilities
  #+BEGIN_SRC sh
  sudo dd if=/dev/xvdf of/dev/null bs=1M
  #+END_SRC
  
- GetMetricStatistics
  - Volume ReadBytes & VolumeWriteBytes
    - The sum statistic reports the total number of bytest transferred
    - Average is also useful to see the average size of each I/O operation
  - VolumeReadOps & VolumeWriteOps
    - Represents the total number of I/O operations
    - You can calculate the average I/O operations per second (IOPS) for a period by dividing the total operations by the number of seconds in that period
  - VolumeTotalTime & VolumeTotalWriteTime
    - The total number of seconds spent by all operations in a given time period
    - A steady increase in these numbers could indicate the need to increase volume size or increase the number of provisioned IOPS
  - VolumeQueueLength
    - Number of read/write operations requests waiting to finish
      
- Provisioned IOPS Metrics
  - VolumeThroughputPercentage
    - The percentage of I/O operations per second that we achieved out of the total perovisioned IOPS for our EBS volume
  - VolumeConsumedReadWriteOps
    - The total amount of read and write operations consumed within a specific time period
      
- EBS Status Checks
  - status checks run every 5 minutes to determine the status of a volume
    - if all checks pass, the status is ok
    - if a check fils, the status is impaired
    - if the checks are running,the status is insufficient-data
      
  - When Amazon EBS finds that data might be inconsistent on a volume it disables I/O to that volume (by default)
    - This helps prevent data corruption
    - It causes a volume status to be impaired which can alert you

*** Lesson 8 - Monitoring RDS for Performance and Availability

- RDS - Monitoring Metrics	
  - CPUUtilization                 - Percentage of CPU utilization
  - DatabaseConnections            - Number of connections that we have at a given point in time
  - DiskQueueDepth                 - Number of read/write requests waiting to access the disk
  - FreeableMemory                 - Amount of available RAM
  - FreeStorageSpace               - Amount of available storage space
  - SwapUsage                      - Increase in this usually has to do with running out of available RAM   
  - ReadIOPS/WriteIOPS             - If not enough IOPS, performance will slow down
  - ReadLatency/WriteLatency       - Higher latency can be solved with more IOPS
  - ReadThroughput/WriteThroughput - Average number of bytes read or written to or from disk per second
    
*** Lesson 9 - Monitoring ElastiCache for Performance and Availability (caching)
    
- ElastiCache supports two engines
  - Memcached
  - Redis
    
- Monitoring Metrics
  - CPU Utilization
  - Evictions
  - CurrConnections
  - Swap Usage (Memcached)
    
- CPU Utilization
  - Memched is multi-threaded
  - Redis is single-threaded
    
  - Memcached
    - Can handle loads of up to 90%
    - Above 90% becomes a problem
    - Solution - vertical or horizontal scaling
      
  - Redis
    - Calculate the threshold: 90/# of CPU cores
    - Solution:
      - For read-heavy workloads, increase the number of read replicas
      - For write-heavy workloads, use a larger cache instance
	
- Evictions
  - Evictions happen when a new item is added but there is no more space. An older item must be deleted to make space.
  - Evictions can be a caching technique used to make sure you don't run out of memory
  - If an items getting evicted too frequently, it defeats the purpose and will decrease performance
  - CloudWatch alarms ccan notify you of a certain threshold
    
  - Memcahed solution - Increase instance size or add nodes to your cluster
  
  - Redis solution - Increase the node size
    
- Current Connections
  - An increase in CurrConnections could indicate a larger problem with your application
    - The app may not be releasing connections
    - Choose a threshold based off of your application requirements
      
- Swap usage (Memcached)
  - swap usage should stay at 0, and not exceed 50MB
  - Swap affects performance and should be avoided
    
  - Solution
    - increase node size
    - increase out ConnectionOverhead parameter value
      
*** Lesson 10 - Monitoring the Elastic Load Balancer for Perdformance and Availability

- Monitoring Metrics
  - Latency 
    - time it takes to receive a response  
    - measure the AVG and MAX values to spot abnormal activity
      
  - BackendConnectionErrors
    - Number of connections that were not successfully established between our load balancer and registered instances
    - Measure SUM and use the different between the minimum and maximums to spot issues
      
  - SurgeQueueLength
    - Measures the total number of requests that are waiting to be routed by the LB
    - Queue can hold a total of 1024 requests
    - Measure the MAX to see the peak of queued requests
    - AVG can also be used with MIN and MAX to get a range
      
  - SpilloverCount
    - if the SurgeQueueLength is full, requests "spill over" and get dropped
    - Measure the SUM
      
  - Pre-warming
    - if you are expecting a sudden and very large increase in traffic, you need to pre-warm your ELB to avoid dropped requests
      
*** Lesson 11 - AWS Billing and Linking AWS Accounts
*** Lesson 12 - AWS Billing Dimensions and Metrics for CloudWatch    
- Once Recieve Billing Alerts is activated it cannot be un-activated
*** Lesson 13 - Cost Optimizing
    
- Save costs by purchasing reserved instances
  
- Reserve instances for 1 to 3 yrs at a discounted rate
  - pay all, in part, or nothing upfront
  - the more you pay upfront, the more you save
    
- Low Utilization
  - save costs by minimizing the number of EC2 instances in-use
  - set ClouldWatch alarms to spin down underutilized instances
    - Example: 5% CPU utilization for 50 minutes
      
  - Find the right balance between availability and cost
  
  - remove unused LB as these are charges per LB
    
  - EBS volumes cost, enven when not in-use
    - delete unused volumes
    - take a snapshot if you want to keep the data
  
  - Provisioned IOPS cost more, make sure you're not provisioning more than necessary
    
  - Downsize volumes that have non-required space
    
  - EIPs cost money, if not in use disassociate them
   
*** Lesson 14 - Using the AWS Price List API and Cost Explorer
*** Lesson 15 - Scalability and Elasticity Essentials    

- What is elasticity?
  - the ability to scale up for demand, then retract back when demand slows down
  - pay only for what yoy need, when you need it
    
- Scalability Fundamentals
  - Scalabiliity focuses meore on building for growth
  - Examples:
    - Increasing instance size
    - Increasing the number of available instances
    - Increasing vol capacity
      
- DynamoDB
  - Scalability
    - we can keep storing more and more data without having to provision any hardware
      
  - Elasticity
    - We can increase or decrease read and write throughput capacity on demand
    - As read requests increase, we can increase read throughput capacity
    - As read requests slow down, we can decrease capacity
      
- EC2
  - Scalability
    - we can increase the size of the instance
    - there are different instance types we can choose from to grow 
    - launch more instances
      
  - Elasticity
    - auto scaling gives the ability to grow with demand, and shrink back during slower periods
      
- RDS
  - Scalabiliity
    - we can increase the size of instaces
    - launch read replicas
    - there are different instance tyeps we can choose from to grow
      
  - Elasticity
    - limited

*** Lesson 16 - Determing Reserved Instance Purchases Based on Business Needs
    
- Reserved Instances
  - Reserved instances give us the ability to purchase instance capacity for a specific period time
  - We can choose standard reserved instances or scheduled reserved
  - Offers discounts
  - Reserves capacity
    
*** Lesson 17 - AutoScaling vs Resizing
    
- Autoscaling
  - distributes the load across multiple instances
  - uses metrics and rules to automate spinning up/terminating instances
    
- Changin instance sizes
  - increases/decreases resources available to our application
    
- When to choose one over the over?
  - they both have pros and cons
    
- Think about if a EC2 Compute Optimized may be more appropriate for the instance type
  
- Scheduled Scaling
  - Auto scaling can scale or shrink on a schedule
    - one time occurrence or recurring schedule
    - can define a new minimum, maximum and scaling size
    - lets you scale out before you actually need capacity in order to avoid delays
      
- Challenges of Auto Scaling
  - relatively complicated to setup
    - instances can be started and stopped at any time
    - applicatiions need to be designed to handle distributed work
    - Important data (sessions, images, etc...) needs to be stored in a central location
    - If one server terminates, the application should still function
  - Delays in scaling
    - Instances take time to initialize
    - Applicatins may require setup which could take even more time

- Challenges of Resizing Instances      
  - Compatibility
    - instances must have the same virt type to resize
    - incompatible instances require migration
  - EBS- backed instances need to be stopped before resize
  - Instance store-backed instances require migration by creating an imamge and launching a new instance from the image
  - Resizing isn't very flexible comparted to Auto scaling
  - There usually has to be downtime and careful planning
  - Resizing instances in Auto Scaling groups may need "suspending"
    
*** Lesson 18 - Elastic Load Balancer Sticky Sessions
    
- Though cookies can be issued with the LB and instances behind, but this may lead to unevenly distributed traffic and ineffect the LB being bypassed due to the cokkies
  
- Elasticache is the prefered method, where the session data could be daved in RDS. This would ensure that the traffic is evenly distributed by the LB.
      
*** Lesson 19 - High Availability with Single Instance Applications that Require Elastic IP Addresses
    
- Problem - older applications moved to AWS might require static IP addresses
  - reasons for this generally include IP addresses hard coded into the code
  - Would require serious commitment to change it
    
- How can you make an application like this highly available and fault tolerant?
  - use an elastic ip (EIP)
  - Understand why Auto scaling will not work
  - create a standby instance in other availability zones
  - increase instance size to scale
    
*** Lesson 20 - Understanding RDS Multi-AZ Failover
    
- RDS Multi-AZ Failover
  - Provisions and maintians a standby replica in a different AZ  
  - The primary synchronously replicates to the standby instance for redundancy
  - Can reduce downtime in the event of a failure on the Primary
    
- How does replication work?
  - The feature can be turned on from the console or API
  - Amazon automatically handles replication
  - The primary instance synchronously replicates to the standby instance for redundancy
  - Replication can cause higher write and commit latency
    - using provisioned IOPS is recommended
      
- Other benefites of replication
  - Patching
    - patching can be done on the standby instance first, and the on the primary to minimize downtime
  - Backups
    - we can eliminate I/O locking and minimize latency spikes
    - create backups from the standby instance
      
- What can trigger a failover?
  - loss of availability in the primary availiability zone
  - loss of network connectivity to the primary instance 
  - resource failure with the underlying virtualized resources
  - storage failure on the primary database
  - the db instance's server type is changed
  - software/OS patching
  - a manual reboot with failover was initiated

- How do failovers work?
  - The Process is automated by AWS
    1. Amazon detects an issue and starts the failover process
    2. DNS records are modified to point to the standby instance
    3. The application re-establishes any existing DB connections
       
*** Lesson 20 - Applying High Availability Bastion Host Instance
    
- Bastion Hosts
  - "Gate" that protects our infrastructure but allows access for updates or other management
  - Used to control remote access (e.g. via RDP or SSH)
  - For inbound traffic exposed to the internet
  - These should be hardened and secured very carefully and reularly updated
    
- Other Benefits
  - can have an Elastic IP Address that never changs and can be whitelisted
  - we can have standby Bastion Hosts for higher availability
    
*** Lesson 21 - Overview of Services that Allow Access to the Underlying Operating System
    
- EMR - Elastic MapReduce
- EC2 - Elastic Cloud Compute
- ECS - Elastic Container Service
- Elastic Beanstalk 
- OpsWorks - Configuration management
  
- Services that don't allow access to the underlying OS
  - RDS
  - DynamoDB
  
*** Lesson 22 - Elastic Load Balancer Configuration
    
- we can have both external and internal LB
  
- External LB are public facing
  - often used to distribute load between web servers
  - provides public DNS hostname
    
- internal load balancer are not customer facing
  - often used to distribure load between private backend servers
  - provides an internal DNS hostname
    
*** Lesson 23 - Offloading Database Workload
    
- RDS Read Replication
  - Read replicas can be used to offlaod work from the main db  
    - writes go to the source instance
    - reads go to the read replicas
      
- Create the read replica    
  - select the source db
  - a snapshot is taken and is applied to the instance that is to become the read replica
      
- RDS Read Replication vs Mutli-AZ failover
  - read replicas are built primarily for performance and offloading work
  - Multi-AZ deployments are used for high availability and durability
  - Multi-AZ deployments give us synchronous replication instaead of asynchronous
  - Multi-AZ deployments are only used to perform a failover, they are idle the rest of the time
  - Read replivas are used to serve legitimate traffic
  - It is often beneficial to use both of these as complements

- which engines to support read replicas
  - innodb
  - extradb
    
  - myisam causes problems, better to use innodb
  
- automated backups has to be initialized for read-replicas to be created
  
*** Lesson 24 - Initializing (Pre-warming)EBS Volumes
*** Lesson 25 - Pre-Warming the Elastic Load Balancer
    
- HTTP 503 Error (ELB cannot handle anymore requests)
  - does not queue requests but instad drops them
    
- ELB dis designed to increase its resource capacity with gradual increases in traffic
  
- When expecting significant spikes in traffic it is possible the traffic is sent faster than the ELB can "expand"
  - contact aws for "pre-warming" of the ELB
    
*** Lesson 26 - Resizing or Changing EBS Root Volume

1 - create a snap shot of the current root volume
2 - with this snap shot choose to create a volume from it
3 - setting a larger size volume will increase the number IOPs available
4 - stop the instance that the new volume is to be attached too
5 - attach the new volume to the stoped instance
6 - restart the instance
7 - ssh into the instance and check that the volume is mounted correctly
    - lsblk or df
    - if full volume not seen use the resize2fs cmd
      
*** Lesson 27 - SSL on Elastic Load Balancer

IAM - should be used if the certs are from a 3rd party
ACM - should be used if the certs are from amazon

*** Lesson 28 - Network Bottlenecks
    
- Potential Issues
  - One of the primary network bottlenecks comes from EC2 instances
  - Instance are in different Availability Zones, regions or continents
  - EC2 instance sizes (larger instances generally have better bandwidth performance)
  - not using enhanced networking features

- performance can be checked with iperf3
  
- VPCs can use VPC peering to create a reliable connection
  - no single point of failure for communication or bandwidth bottlenecks
    
- using iperf3 to monitor/bench mark networking
#+BEGIN_SRC sh
iperf3 -s -p 80
#+END_SRC
p - 80
- on another instance install iperf3 (ubuntu instance in this case), if not available in repo of distro it is available from github
#+BEGIN_SRC sh
apt-get install iperf3
#+END_SRC
- on this instance connect back to the instance we are testing
#+BEGIN_SRC sh
iperf3 -c 53.234.170.10 -i 1 -t 10 -p 80
#+END_SRC
c - connect
i - interval
t - duration of time
This will provide detailed information of each interval and an overall sender/reciever bandwidth

- Bandwidth limitations on your VPN to your AWS VPC
  - Using VPN to access AWS VPC from our on-premise network means we have to communicate over the open internet
    
- We can use AWS Direct Connect
  - Gives us a dedicated network connection
  - sets up a private connection
  - can reducee costs in some situations 
  - supports post speeds of 1Gbps and 10Gbps
  - Speeds of 50Mbps, 100Mbps, 200Mbps, 300Mbps, 400Mbps and 500Mbps can be ordered through an APN Partner supporting AWS Direct Connect

*** Lesson 29 - Lab - Test Bandwidth on EC2 instances with iperf3
*** Lesson 30 - EBS Root Devices on Terminated Instances - Ensuring Data Durablility
    
- delete on termination is set a default
  - for persistance this should be unticked
    
- backing up data
  - uncheck the delete on termination
  - create a snapshot before you terminate the instance
  - create a volume to backup other volumes too.   

*** Lesson 31 - Troubleshooting Auto Scaling Issues
    
- Attempting to use the wrong subnet
- Availability is no longer available or supported
- Security group does not exist
- Key pair associated does not exist
- Auto scaling configuration is not working correctly
- Instance type specification is not supported in that Availability Zone
- Auto Scaling service is not enabled on the account
- Invaild EBS device mapping
- Attempting to attach EBS block device to instace-store AMI
- AMI issues
- Placement group attempting to use m1.large (wrong instance type)
- "We currently do not have sufficient instance capacity in the AZ that you requested"
- Updating instance in Auto Scaling group with "suspended state" 
  
*** Lesson 32 - OpsWorks: Overview
    
- What is OpsWorks
  - give us flexible way to create and manage resources for our applications, as well as the applications themselves.
  - we can create a stack of resources and manage those resources collectively in different layers. These layers can have built-in or suctom Chef recipes.    

    - Overall
      - automate deployments
      - monitor deployments
      - maintain deployments

  - Anatomy
    - Stacks
      - represent a set of resources that we want to manage as a group
        - e.g EC2 instances, EBS volumes, LB
      - We could build a stack for a development, staging or production environment
	
    - Layers
      - Used to represent and configure components of a stack
        - e.g. a layer web app servers, a layer for the db, and a layer for the LB
      - we can use built-in layers and customize those or create completley custom layers
      - recipes are added to layers
	
    - Instances
      - must be associated with at least one layer
      - we could build a stack for a development, staging, or production environment
      - we can run as:
        - 24/7
        - load-based
        - time-based

    - Apps
      - Apps are deployed to the application layer through a source code repo likt Git, SVN or seven S3.
      - We can deploy an app against a layer and have ops works exec recipes to prepare instancees for the app.
	
Layer  -----  LB

Layer  -----  Instances

Layer  -----  DB Instance

  - Recipes
    - created using the ruby language and based off of the chef deployment software
    - custom recipes can customize different layers in an application
    - recipes are run at certain per-defined events within a stack
      - Setup - occurs on a new instance after first boot
      - Configure - occurs on all stack instances when they enter or leave the online state
      - Deploy - occurs when we deploy an app
      - Undeploy - happens when we delete an app from a set of application instances
      - Shutdown - happens when we shut down an instance (but before it is actually stopped)

*** Lesson 33 - OpsWorks: Creating our First Stack
*** Lesson 34 - CloudFormation Essentials    
    
- CloudFormation allows you to create and provision resources in a reusable template fashion
- turns your resources into stacks that work as units
- allows you to source control your infrastructure
- templates are JSON compatible
  
- Version and Description
  - AWSTemplateFormatVersion
    - Specifies which template version you want to use
  - Description
    - This section follow the template version section
    - Descriptions help clearly differentiate between templates
  - Metadata
    - JSON objects that provide details about the template
  - Parameters
    - Valuees you can pass in right before template creation
    - allows you to customize templates
    - can have default values as well as allowed values
  - Mappings
    - Lets you map keys to values
    - for example: you can make different valuees for different regions
  - Conditions
    - Can check values before deciding what to do
    - Allows you to create different resources in the same template depending on the condition evaluation
    - Example: can create different environments for development and production
  - Resources (required)
    - this is where you create different resources
  - Outputs
    - can ouptu values that you'd like see from the console of from API calls
      
  - Intrinsic Functions
    - used to pass in values that are not avaklable until run time
    - Fn::GetAtt 
    - Fn::FindInMap - redturns the value of a key from a specified mapping
    - Fn::Join - Concat elements, separated by a specified delimiter
    - Ref - Returns a resource or value based on a logical name or parameter
    - Fn::GetAZs - Get the AZ for a specified region
    - Fn::Select - Returns a single object from a list of objects by index
    
  - CloudFormation Rollback
    - if a stack fails to create a resource, by default the stack will rollback
    - Rollback - Removal of all created resources after a failed stack creation, or after cancelling creation
    - Rollback can be disabled from the API
      
  - Advanced Concepts
    - templates allow you to declare cloud-init scripts for EC2 resources
    - templates allow the use of regex in certain declarations

*** Lesson 35 - Backup Services on AWS and Services that Include Backups

- RDS backups
  - transaction storage engine is recommended for durability
  - degrades performance if Multi-AZ is not enabled
  - Deleting an instance deletes all automated backups (not manual backups)
  - Backups are stored internally on Amazon S3
    
- RDS restoring
  - When restoring, only the default DB parameter and security groups are associated with the instance
  - You can change to a different DB engine as long as it is closely related to the previous engine and there is enough space allocated
    
- ElastiCache
  - Backups available for Redis clusters only
  - Snapshots backup data for the entire cluster at a spcific point in time
  - Backup window should be during the least-utilized time period of the day
  - Snapshots can degrade performance and should be perfomance on read replicas
    
- Redshift
  - Provides free storage equal to the storage capacity of the cluster
  - Snapshots can be automated or manual, and incremental
  - Restoring snapshots creates a new cluster and imports the data
    
- EC2
  - No built-in automated backup option
  - Snapshots of EBS volumes are incremental and can be automated with the API, CLI, or even AWS Lambda
  - Snapshots cause performance degradation 
  - snapshots are stored on S3
    
*** Lesson 36 - Creating and scripting Automation for EC2 SnapshotsC
    
- packages requred for the script
#+BEGIN_SRC sh
yum install python-pip
pip install boto3
#+END_SRC

- aws configure - settings that the script will use
  - this ceates the .aws/credentials that scripts will use
  #+BEGIN_SRC sh
  aws configure
  #+END_SRC
  - It will prompt for Access Key ID, Secret Access Key, Default Region and Default output format
  - If aws-cli isn't install on the instance the file can be created manually
  ~/.aws/credentials
  #+BEGIN_EXAMPLE
  [default]
  aws_access_key_id = AKIAIMOUZUR4MYVEJCNASE
  aws_secret_access_key = 4hw3RIFMemoI4ffWmbscV2MA28zGpSRyx/
  #+END_EXAMPLE
  
- backup_all_vols.py
#+BEGIN_EXAMPLE
#!/usr/bin/python

import boto3

ec2 = boto3.resource('ec2')

for volume in ec2.volumes.all():
	vol_id = volume.id
	description = "backup-%s" %(vol_id)
	ec2.create_ssnapshot(Volume(d=vol_id, Description=description)
#+END_EXAMPLE

- backup_only_running_vols.py 
#+BEGIN_EXAMPLE
#!/usr/bin/python

import boto3

ec2 = boto3.resource('ec2')

print("\n\nAWS snapshot backup started")
instances = ec2.instances.filter(
	Filters=[{'Name': 'instance-state-name', 'Values': ['running]}])
	
for instance in instances:
	instance_name = filter(lambda tag: tag['Key'] == 'Name', instance,tags)[0]['value']
	
	for volume in ec2.voumes.filter(Filters=[{'Name': 'attachment.instance-id', 'Values':[instance.id]}]):
		description = 'scheduled_snapshot-%s.%s %(instance_name, volume.volume_id)
		
	if volume.create_snapshot(VolumeID=volume_id, Description=description):
		print("Snapshot created with description [%s]" % description)
		
print("\n\nAWS snapshot backups completed")
#+END_EXAMPLE

- backup_retention_check.py
backs up all volumes then checks the age of the volumes and deletes any that are passed retention period
#+BEGIN_EXAMPLE
#!/usr/bin/python

import boto3
import datetime
import pytz

ec2 = boto3.resource('ec2')

print("\n\nAWS snapshot backup started %s" % datetime.datetime.now())
instances = ec2.instances.filter(
	Filters=[{'Name': 'instance-state-name', 'Values': ['running]}])
	
for instance in instances:
	instance_name = filter(lambda tag: tag['Key'] == 'Name', instance,tags)[0]['value']
	
	for volume in ec2.voumes.filter(Filters=[{'Name': 'attachment.instance-id', 'Values':[instance.id]}]):
		description = 'scheduled_snapshot-%s.%s-%s' %(instance_name, volume.volume_id, datetime.datetime.now().strftime("%Y%m%d-%H%M%S"))
		
	if volume.create_snapshot(VolumeID=volume_id, Description=description):
		print("Snapshot created with description [%s]" % description)
		
	for snapshot in volume.snapshots.all():
		retention_days = 15
		if snapshot.description.startswith('scheduled_snapshot-') and ( datetime.datetime.now().replace(tzinfo=None) = snapshot.stat_time.replace(tzinfo=None) ) > datetime.timedelta(days=retention_days):
			print("\t\tDeleting snapshot [%s - %s]" % (snapshot.snapshot_id, snapshot.description))
			snapshot.delete()
		
print("\n\nAWS snapshot backups completed")
#+END_EXAMPLE

*** Lesson 37 - Read Replicas with MySQL RDS Across Regions

RDS Read Replicas Across Regions
  - Disaster recovery
    - Multi-AZ deployments are not enough to protect against entire regions going down
    - We can use read replicas in other regions for HA
  - Cross-reion replicas can help with performance if we have a global audience
	- packets have a shorter distance to travel between DB and the end user
  - Replica lag can be expected to go up since data has to go across regions

The process of setting up Read Replicas can take quiet a bit of time, this is something that is a fore-thought in case of disaster, not something that can be done to avert disaster within minutes.

*** Lesson 38 - Quickly Recoving from Disasters

- A disaster - anything that has a negative impact on business continuity or finances
- if an entire region goes down, how can you recover as quickly as possible?
  - We can use read replicas across regions for our DB
  - We can have a backup to our infrastructure in a geographically seperate location
  - We can have the latest data and configuration available on our backup
	
- Costs
  - Backup resources sit idle and therefore add to our costs
  - With AWS we only pay for the resources that we use
  - We can lower our costs by only provisioning the bare minimum
	- E.g. Run fewer instances but configure Auto Scaling to automatically grow if needed
	  
- Services for on-premises infrastructure with AWS
 - EC2 and EBS
 - S3
 - AWS Import/Export Snowball (large data movement, disks set to you to post back)
 - RDS
 - ELB and Auto Scaling
 - Amazon Storage Gateway (backup data to S3 automatically)
   - Virtual Tape Library
 - CloudFormation
   
- Tools for Recovery
  - EC2 AMIs
  - VM Import/Export
  - For VMWare - we can use the AWS Management Portal for vCenter
  - Direct Connect - on-premises ppp link you and amazon, good for a lot of data but not as much as snowball
  - Amazon S3 Transfer Acceleration
	
- potential issues with replicating data	
  - The distance between our replication sites ca nincrease replica lag
  - Bandwidth limitations can also delay data replication
  - It's important to understand which services have async replication and which have sync replication
	
*** Lesson 39 - Storing Log Files and Backups
	
- Storing Log Files and Backups
  - Centralized logging
	- consolidated logs in one central location
	- analyze, store and modify the data in any way that you need

  - Tools
	- [[file://home/crito/org/tech/monitoring/rsyslog.org][Rsyslog]] is a good tool for this
	- Splunk
	- Kiwi
	- Graylog
	- [[file://home/crito/org/tech/monitoring/elk_stack.org][ELK Stack]]
	  
  - Other types of logging
	- S3 access logs
	  - enable logging on a bucket
      - Requests made to that bucket will be logged and stored on S3
	  - No extra charge, except the extra storage cost for the logs
		
    - CloudTrail
	  - Logs API calls made on our account
	  - Useful for debugging, security auditing, and to learn how users interact with our resources
		
    - CloudWatch logs
	  
*** Lesson 40 - S3 IAM and Buck Policies Concepts
	
- Amazon S3 IAM Policies and Bucket Policies
  - IAM Policy
	- Applies to the user level
	- "User" policy
	  
  - Bucket Policy
	- Applies to the resource level
	- "Resource-based" policy
	  
  - S3
	- Can use buket and user policies
	  - resource-based policies
	  - user policies
    - Bucket permissions specify:
      - who is allowed to access resources
	  - what that user can do with those resources
	- AWS gives full permissions to the owner of a resource
    - Resource owners can grant access to others, even cross-account
	  - The bucket owner paying bills can deny access or modify objects regardless of who owns it
		
  - Bucket Policies
	- resource-based policy
	- used a json file attached to the resource
    - can grant other aws accounts or IAM users persision for the bucket and objects inside
	- should be used to manage ross-account permissions for all Aazon S3
	- limited to 20KB in size
	- Example Bucket Policy  
    #+BEGIN_EXAMPLE
    {
	 "Version":2015-10-02",
	 "Statement": [
	  {
	   "Sid": "AddObject",
	   "Effect": "Allow",
	   "Principal": {"AWS": ["arn:aws:iam::862345521403:user/james"]},
	   "Action": ["s3:PutObject"],
	   "Resource": "arn:aws:s3:::examplebucket/*"
	  }
     ]
	}
    #+END_EXAMPLE
	
  - ACLs
	- used for both buckets and objects
	- grant read/write permissions to other AWS accounts
	- you cannot grant conditional permissions
	- you cannot explicitly deny permissions
	- an object ACL is the only way to manage access to objects not owned by the bucket owner
	- use XML format
	  
  - IAM policies (user-based)
	- user policy
	- can create multiple users and give them the same policy or different policies
	- policies are attached and can be detached
	- cannot grant anonymous users
	- Example IAM policies
	  #+BEGIN_EXAMPLE
	  {
	   "Statement": [
	    {
		 "Effect":"Allow",
		 "Action": [
		  "s3:PutObject",
		  "s3:GetObject",
		  "s3:DeleteObject",
		  "s3:ListAllMyBuckets",
		  "s3:ListBucket"
		 ],
		 "Resource2:"arn:aws:s3:::examplesbucket/*"
	    }
	   ]
	  }
	  #+END_EXAMPLE
	  
  - Specifying Resources in a policy
	- arn:aws:s3:::bucket_name
	- arn:aws:s3:::bucket_name/key_name
	  
    - All object in examplebucket
	- arn:aws:s3:::examplebucket/*

	- All buckets
	- arn:aws:s3:::*

	- Variables
	- arn:aws:s3:::examplebucket/developers/${aws:username}/
	  
*** Lesson 40 - Bucket Policies
	
- Elements of an access policy
  - Resources
    - used to idenfity resources with amazon resource names (ARN)
	  
  - Actions
	- actions we want to allow or deny
	- explicit deny always overfides an explicit allow
	  
  - Effect
	- defines whether to allow or deny the above action
	
  - Principal
	- an account or user that this policy applies
	- specific to S3 bucket policies, not user policies
	  
*** Lesson 41 - Building IAM Policies
	




- IAM Policy Simulator
  - this allows you to check your policies against set actions
	
*** Lesson 42 - Network Access Control Lists (NACLs) and Security Groups
	
- VPC Secuirty
  - Security groups
  - NACLs
	
*** Lesson 42 - Using IAM Roles with EC2
*** Lesson 43 MFA on Amazon Web Services(Multifactor Authentication)	
	
- AWS MFA to access the console
  - users type in their user and passsword as well as a time-based code
  - the time-based code can be on the user's computer, smartphone, or a device that they carry around
  - this should be turned on for the users who have access to the console
	
- Enable MFA for API access
  - you can protect your resources from unauthorised API calls using MFA
  - with IAM and bucket policies, we can decide which actions require this and for which resources
	
- Integrating MFA with Amazon STS
  - we need to integrate with the security Token Service to receive temporary credentials
	- to do that, our call should include the device identifier for the device associated with our account
	- we also need to include the time-based code generated by our device
	- we then get back our temporary security cedentials that can be used to make requests against AWS Services
  - Policies can check for the presence of the MFA policy or they can force periofic re-authentication
  - Not all services support this - services like Amazon S3, SQS and SNS do support it
	
*** Lesson 43 - Security Token Service
	
- AWS Security Token Service
  - allos you to grant trusted user temporary and controlled access to AWS resources
	
  - Grant temporary access
	- to existing IAM users
	- to web-based identity providers: Facebook|Amazon|Google
	- to your organization's existing identity system
	  
  - Credentials are associated with an IAM access control policy that limits what the user can do
	
  - Amazon STS API
	- AWS SDKs
	- AWS CLI
	- AWS Tools for Windows Powershell
	  
  - STS
	- STS returns temporary security credentials
	  - these consist of an access key and a session token
	- Access key
	  - consists of an access key ID and a secret key 
    - Session Token
	  - used to validate our user's temporary security credentials
	- Credentials expire after a certain amount of time

  - Terms
	- Federation
	  - creating a truct relationship between an identity provider and AWS
	  - Users can sign into an identity provider like Amazon, FB, Google, or any other recognized provider
	- Identity broker
	  - The broker is in charge of mapping the user to the right set of credentials
	- Identity Store
	  - An identity store is something like FB, Google, Amazon or AD
	- Identities
	  - A user or "identity" within an identity store
		
  - Temporary Credentials with Amazon EC2
	- Assign an IAM role to the EC2 instance
	- Get automatic temporary security credentials from the instance metadata using the AWS SDKs/CLI
	- You don't have to explicitly get credentials
	  
*** Lesson 44 - Shared Responsibility Model
	
- Shared Responsibility Env (your end)
  - IAM
  - MFA
  - Password/Key Rotation
  - Access Advisor
  - Trusted Advisor
  - Security Groups
  - Access Control Lists
  - VPC
	
- Shared Responsibility Env (AWS)
  - pyhsical server level and below
  - physical environment security and protection - /fire/power/climate/management
  - storage device decommissioning according to industry standards
  - Network device security and ACL's
  - API access endpoints use ssl for secure communication
  - ddos protection
  - EC2 instances cannot send spoofed data

- port scanning against rules even if it's your own environment 	
- personel access to facilities

- EC2 instance hypervisor isolation  
  - even if instances are on the same physical device, thy are separated at the hypervisor level. They are independent of each other.

*** Lesson 45 - AWS and IT Audits
	
- AWS performs self audits of changes to key services to monitor quality, maintain high stantards, and facilitate continuous improvement of the change management process
  
- For audits, AWS provides:
  - Information regarding their global infrastructure
  - from the host OS and virt layer down to the physical security of facilities
  - AWS provides annual cert and reports: (SOC (Service Organization Control) reports, ISO 27001 cert, PCI assessments)
	
- For audits, the customer provides:
  - anything their organization puts on their AWS assets
  - e.g. OS, apps on VM instances, objects in S3, DB like RDS etc
	
*** Lesson 46 - Route53 and DNS Failover	
*** Lesson 47 - Weighted Routing Policies in Route53	
	
- This ability allows you to determine where traffic is sent based on the DNS settings
- This sits in front of the ELB
  
- good for slow migration to new version of application (70/30, 80/20, 90/10, 100/0)
  
*** Lesson 48 - Latency Based Routing
	
- This is used for multiple region infrastructure
- This uses regions to know what latency to set for the user

*** Lesson 49 - VPC Essentials
	
- VPC resembles
  - private data centers
  - private corporate networks
	
- private network
  - private and public subnets
  - scalable infrastructure
  - ability to extend corporate/home network to the cloud as if it were part of your network
	
- Benefits of a VPC
  - Ability to launch instances into a subnet
  - Ability to define custom ip addr ranges inseide of each subnet
  - Ability to configure route tables between subnets
  - Ability to create a layered network of resources
  - Extending our network with VPN/VPG controlled access 
  - Ability to use Security Groups and Subnet network ACLs
	
- Default VPC
  - default VPC is a different setup than a non-default VPC
  - Default VPC gives users easy access to a VPC without having to configure it from scratch
  - Default VPC subnets have internet gateways attached
  - Each instance added has a default private and public IP address
  - If you delete the default VPC, the only way to get it back is to contact AWS
	
- non-default
  - non-default vpv have private ip addr but not pub ip addr
  - can only access resources through elastic ip addr, VPNs or gateway instances
  - do not have internet gateways attached by default
	
- VPC peering
  - vpc peering allows you to setup direct network routing between different vpc using private ip addr
  - instances will communicate with each other as if thery were on the same private network
  - vpc peering can occur between other AWS accounts and other VPCs within the same region
	
- VPC Limits
  - 5 VPCs per person
  - 200 subnets per VPC
  - 50 Customer gateways per regiion
  - 5 internet gateways per region
  - 5 elastic ip addr per region for each AWS account
  - 50 VPN connections per region
  - 200 route tables per region
  - 500 security groups per region
	
*** Lesson 50 - Building a Non-Default VPC
	
- Don't delete the default VPC, you will have to contact AWS to get a new one
- makesure to use ssh-add, to dperform ssh forwarding to private instances through public instances  
  
*** Lesson 51 - VPC Networking
*** Lesson 52 - VPC Security
	
  Internet Gateway

        Router
		
     Route Table
  
     Network ACL
  
        Subnet
  
   Security Group
   
- Above is the flow of traffic and how security is implemented
- a subnet has to have a acl attached and will use the default if it is the only one available
  

*** Lesson 53 - Configuring a NAT Instance
	
- this instance routes traffic from the private instances to the internet
  - this will allow outside connection to private instances
  - the update of private instances from the external sources(internet/git)

- a special security group needs to be created for the NAT instance
  - the ip table rules need to be set: 
    - to allow the private instances to connect to any external port
	- to allow the private instances ip/subnet to be able to connect
	  
- the source/destination check needs to be disabled on the NAT instance
  
*** Lesson 54 - DB Subnet Groups
*** Lesson 55 - Elastic IP Addresses and Elastic Network Interfaces	
*** Lesson 56 - Configuring Web Appliction in a Non-Default VPC	
	
- first buld the non-default VPC
  - create subnets
	- public and private
  - don't forget about Multi-AZ for failover
  - attach the internet gateway	
	- add route to public subnet
	  
- launch RDS
  - set DB Subnet Group

- launch EC2 instance
  - use git to clone app into new instance
  - use the dep tool to install deps (composer, pip etc)	
  - connect instance to RDS
	- methods for connection will vary with different lang, platform (laravel use .env)
  - Use this instance to create an AMI
	- this AMI we will deploy into the auto-scaling group
	  
- set up security groups for the EC2 to connect to RDS, otherwise the connection will fail
  
- confirm nginx is running
  - move application to correct directory and configure nginx to server application
    - choose between Fastcgi or php-fpm
    - ensure that permissions are correct on the application	
	  
- Create an internate facing LB
  - you will need to ensure that each AZ has a public subnet
  - configure health check setting
	
- Configure Auto-Scaling

*** Lesson 57 - AWS Direct Connect and On-premises to VPC Redundancy
	
- you can connect on-site infrastructure to AWS
  - move business apps to the cloud
  - run analytics

- it is achieved by using VPN
  - adding a Virtual Private Gateway to the VPC that you can connect customer network.
	
- Considerstions
  - you can have 5 VPG per region
  - you can only have 1 VPG per VPC
  - you can have 50 Customer Gateways per region
  - these numbers can be increased by AWS
	
- Bandwidth Considerations
  - most vpn connections cannot support consistent 4Gbps data transfer rates
  - AWS direct connect offers dedicated network connections
	- more badnwidth throughput
	- consistent performance
	- private connection instead of going over the public internet
	- direct connect provides 1Gbps and 10Gbps ports and we can provision multiple connections if we need more capacity

- AWS Direct Connect uses BGP drouting
  - we need to use BGP with ASN and IP prefixes
	
- Creating redundat tunnels
  - if something happens to our first tunnel, we can automatically failover to the second
    - one tunnel is always used and the other is for failover only
    - the customer Gateway must be configured for both tunnels
	  




  



* Books
** SysOps Associate
[[file://home/crito/Documents/SysAdmin/Cloud/AWS/sysops/AWS_Auditing_Security_Checklist.pdf][AWS Auditing Security Checklist]]
[[file://home/crito/Documents/SysAdmin/Cloud/AWS/sysops/AWS_Backup_Recovery.pdf][AWS Backup Recovery]]
[[file://home/crito/Documents/SysAdmin/Cloud/AWS/sysops/AWS_Building_Fault_Tolerant_Applications.pdf][AWS Building Fault Tolerant Applications]]
[[file://home/crito/Documents/SysAdmin/Cloud/AWS/sysops/AWS_certified_sysops_associate_blueprint.pdf][AWS Certified SysOps Associate Blueprint]]
[[file://home/crito/Documents/SysAdmin/Cloud/AWS/sysops/AWS_Cloud_Architectures.pdf][AWS Cloud Architectures]]
[[file://home/crito/Documents/SysAdmin/Cloud/AWS/sysops/AWS_Disaster_Recovery.pdf][AWS Disaster Recovery]]

* Links
